{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5506dd7",
   "metadata": {},
   "source": [
    "# DocFrame Demo: Text Analysis Made Simple\n",
    "\n",
    "Welcome to DocFrame! This notebook demonstrates how DocFrame makes text analysis in Python as simple and powerful as possible by combining the performance of Polars with intelligent text processing capabilities.\n",
    "\n",
    "## What is DocFrame?\n",
    "\n",
    "DocFrame extends Polars DataFrames with document-aware functionality, automatically detecting text columns and providing unified text processing APIs. It bridges the gap between raw data manipulation and text analysis.\n",
    "\n",
    "### Key Features:\n",
    "- ğŸš€ **Automatic Document Detection**: Intelligently identifies your main text column\n",
    "- ğŸ”„ **Unified Text API**: Same text operations work on Series, DataFrames, and Expressions  \n",
    "- âš¡ **Performance**: Built on Polars for speed and memory efficiency\n",
    "- ğŸ¯ **Seamless Integration**: Drop-in replacement that enhances your existing Polars workflows\n",
    "- ğŸ“Š **Rich Text Analytics**: Built-in text statistics, cleaning, and transformation tools\n",
    "\n",
    "Let's explore using real political data from the 2020 Queensland election!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59aae25",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d1f4721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2_380, 15)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>tweet_id</th><th>created_at</th><th>retrieved_at</th><th>user_id</th><th>username</th><th>text</th><th>retweeted_tweet_id</th><th>retweeted_user_id</th><th>retweeted_user_name</th><th>in_reply_to_tweet_id</th><th>in_reply_to_user_id</th><th>in_reply_to_user_name</th><th>quoted_tweet_id</th><th>quoted_user_id</th><th>quoted_user_name</th></tr><tr><td>i64</td><td>str</td><td>str</td><td>i64</td><td>str</td><td>str</td><td>i64</td><td>i64</td><td>str</td><td>i64</td><td>i64</td><td>str</td><td>i64</td><td>i64</td><td>str</td></tr></thead><tbody><tr><td>1317123226535776257</td><td>&quot;2020-10-16 15:20:22.000 +0000&quot;</td><td>&quot;2020-10-17 02:01:46.000 +0000&quot;</td><td>162876955</td><td>&quot;JoelRichtersALP&quot;</td><td>&quot;RT @GhostWhoVotes: #Newspoll Qâ€¦</td><td>1317038311504924672</td><td>56077211</td><td>&quot;GhostWhoVotes&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>1317123482837086208</td><td>&quot;2020-10-16 15:21:23.000 +0000&quot;</td><td>&quot;2020-10-17 02:01:46.000 +0000&quot;</td><td>162876955</td><td>&quot;JoelRichtersALP&quot;</td><td>&quot;RT @camerondickqld: And @DebFrâ€¦</td><td>1317006327500001281</td><td>236818401</td><td>&quot;camerondickqld&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>1317123564248551425</td><td>&quot;2020-10-16 15:21:43.000 +0000&quot;</td><td>&quot;2020-10-17 02:01:46.000 +0000&quot;</td><td>162876955</td><td>&quot;JoelRichtersALP&quot;</td><td>&quot;RT @QLDLabor: Different electiâ€¦</td><td>1316997215064268801</td><td>147861172</td><td>&quot;QLDLabor&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>1317123704678019072</td><td>&quot;2020-10-16 15:22:16.000 +0000&quot;</td><td>&quot;2020-10-17 02:01:46.000 +0000&quot;</td><td>162876955</td><td>&quot;JoelRichtersALP&quot;</td><td>&quot;RT @gracextwo: Access to sanitâ€¦</td><td>1316975344293871616</td><td>263418376</td><td>&quot;gracextwo&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>1317124150243151874</td><td>&quot;2020-10-16 15:24:03.000 +0000&quot;</td><td>&quot;2020-10-17 02:01:46.000 +0000&quot;</td><td>162876955</td><td>&quot;JoelRichtersALP&quot;</td><td>&quot;RT @MarkBaileyMP: Clive Palmerâ€¦</td><td>1316888481939877890</td><td>984808496</td><td>&quot;MarkBaileyMP&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>1323363743862935552</td><td>&quot;2020-11-02 20:37:58.000 +0000&quot;</td><td>&quot;2020-11-02 21:45:21.000 +0000&quot;</td><td>1298023708859052032</td><td>&quot;DanielleShankey&quot;</td><td>&quot;@AnnastaciaMP and her team areâ€¦</td><td>null</td><td>null</td><td>null</td><td>null</td><td>256833444</td><td>&quot;AnnastaciaMP&quot;</td><td>1323181552302514176</td><td>256833444</td><td>&quot;AnnastaciaMP&quot;</td></tr><tr><td>1323364106334728193</td><td>&quot;2020-11-02 20:39:24.000 +0000&quot;</td><td>&quot;2020-11-02 21:15:19.000 +0000&quot;</td><td>452670762</td><td>&quot;philip__anthony&quot;</td><td>&quot;Another from Wardill in today&#x27;â€¦</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>1323366133596999680</td><td>&quot;2020-11-02 20:47:27.000 +0000&quot;</td><td>&quot;2020-11-02 21:15:19.000 +0000&quot;</td><td>452670762</td><td>&quot;philip__anthony&quot;</td><td>&quot;And Wardill keeps it coming inâ€¦</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>1323372798174089216</td><td>&quot;2020-11-02 21:13:56.000 +0000&quot;</td><td>&quot;2020-11-02 22:24:24.000 +0000&quot;</td><td>984808496</td><td>&quot;MarkBaileyMP&quot;</td><td>&quot;RT @camerondickqld: ABS data iâ€¦</td><td>1323115321381715968</td><td>236818401</td><td>&quot;camerondickqld&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>1323373512308871168</td><td>&quot;2020-11-02 21:16:47.000 +0000&quot;</td><td>&quot;2020-11-02 22:24:24.000 +0000&quot;</td><td>984808496</td><td>&quot;MarkBaileyMP&quot;</td><td>&quot;@CampbellNewman @AlboMP @QLDLaâ€¦</td><td>null</td><td>null</td><td>null</td><td>1323107059202625537</td><td>531881411</td><td>&quot;CampbellNewman&quot;</td><td>null</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "DocDataFrame(shape: (2_380, 15)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ tweet_id  â”† created_a â”† retrieved â”† user_id   â”† â€¦ â”† in_reply_ â”† quoted_tw â”† quoted_us â”† quoted_u â”‚\n",
       "â”‚ ---       â”† t         â”† _at       â”† ---       â”†   â”† to_user_n â”† eet_id    â”† er_id     â”† ser_name â”‚\n",
       "â”‚ i64       â”† ---       â”† ---       â”† i64       â”†   â”† ame       â”† ---       â”† ---       â”† ---      â”‚\n",
       "â”‚           â”† str       â”† str       â”†           â”†   â”† ---       â”† i64       â”† i64       â”† str      â”‚\n",
       "â”‚           â”†           â”†           â”†           â”†   â”† str       â”†           â”†           â”†          â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ 131712322 â”† 2020-10-1 â”† 2020-10-1 â”† 162876955 â”† â€¦ â”† null      â”† null      â”† null      â”† null     â”‚\n",
       "â”‚ 653577625 â”† 6 15:20:2 â”† 7 02:01:4 â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 7         â”† 2.000     â”† 6.000     â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚           â”† +0000     â”† +0000     â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 131712348 â”† 2020-10-1 â”† 2020-10-1 â”† 162876955 â”† â€¦ â”† null      â”† null      â”† null      â”† null     â”‚\n",
       "â”‚ 283708620 â”† 6 15:21:2 â”† 7 02:01:4 â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 8         â”† 3.000     â”† 6.000     â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚           â”† +0000     â”† +0000     â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 131712356 â”† 2020-10-1 â”† 2020-10-1 â”† 162876955 â”† â€¦ â”† null      â”† null      â”† null      â”† null     â”‚\n",
       "â”‚ 424855142 â”† 6 15:21:4 â”† 7 02:01:4 â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 5         â”† 3.000     â”† 6.000     â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚           â”† +0000     â”† +0000     â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 131712370 â”† 2020-10-1 â”† 2020-10-1 â”† 162876955 â”† â€¦ â”† null      â”† null      â”† null      â”† null     â”‚\n",
       "â”‚ 467801907 â”† 6 15:22:1 â”† 7 02:01:4 â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 2         â”† 6.000     â”† 6.000     â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚           â”† +0000     â”† +0000     â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 131712415 â”† 2020-10-1 â”† 2020-10-1 â”† 162876955 â”† â€¦ â”† null      â”† null      â”† null      â”† null     â”‚\n",
       "â”‚ 024315187 â”† 6 15:24:0 â”† 7 02:01:4 â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 4         â”† 3.000     â”† 6.000     â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚           â”† +0000     â”† +0000     â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ â€¦         â”† â€¦         â”† â€¦         â”† â€¦         â”† â€¦ â”† â€¦         â”† â€¦         â”† â€¦         â”† â€¦        â”‚\n",
       "â”‚ 132336374 â”† 2020-11-0 â”† 2020-11-0 â”† 129802370 â”† â€¦ â”† Annastaci â”† 132318155 â”† 256833444 â”† Annastac â”‚\n",
       "â”‚ 386293555 â”† 2 20:37:5 â”† 2 21:45:2 â”† 885905203 â”†   â”† aMP       â”† 230251417 â”†           â”† iaMP     â”‚\n",
       "â”‚ 2         â”† 8.000     â”† 1.000     â”† 2         â”†   â”†           â”† 6         â”†           â”†          â”‚\n",
       "â”‚           â”† +0000     â”† +0000     â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 132336410 â”† 2020-11-0 â”† 2020-11-0 â”† 452670762 â”† â€¦ â”† null      â”† null      â”† null      â”† null     â”‚\n",
       "â”‚ 633472819 â”† 2 20:39:2 â”† 2 21:15:1 â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 3         â”† 4.000     â”† 9.000     â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚           â”† +0000     â”† +0000     â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 132336613 â”† 2020-11-0 â”† 2020-11-0 â”† 452670762 â”† â€¦ â”† null      â”† null      â”† null      â”† null     â”‚\n",
       "â”‚ 359699968 â”† 2 20:47:2 â”† 2 21:15:1 â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 0         â”† 7.000     â”† 9.000     â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚           â”† +0000     â”† +0000     â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 132337279 â”† 2020-11-0 â”† 2020-11-0 â”† 984808496 â”† â€¦ â”† null      â”† null      â”† null      â”† null     â”‚\n",
       "â”‚ 817408921 â”† 2 21:13:5 â”† 2 22:24:2 â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 6         â”† 6.000     â”† 4.000     â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚           â”† +0000     â”† +0000     â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 132337351 â”† 2020-11-0 â”† 2020-11-0 â”† 984808496 â”† â€¦ â”† CampbellN â”† null      â”† null      â”† null     â”‚\n",
       "â”‚ 230887116 â”† 2 21:16:4 â”† 2 22:24:2 â”†           â”†   â”† ewman     â”†           â”†           â”†          â”‚\n",
       "â”‚ 8         â”† 7.000     â”† 4.000     â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚           â”† +0000     â”† +0000     â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜, document_column='text')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "import docframe\n",
    "candidates_df = pl.read_csv(\"data/ADO/candidate_info_gender.csv\")\n",
    "tweets_df = docframe.read_csv(\"data/ADO/qldelection2020_candidate_tweets.csv\")\n",
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7f846a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (79, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>username</th><th>text</th></tr><tr><td>str</td><td>u32</td></tr></thead><tbody><tr><td>&quot;meaghan_scanlon&quot;</td><td>1</td></tr><tr><td>&quot;Ros_Bates_MP&quot;</td><td>1</td></tr><tr><td>&quot;Bennett4Burnett&quot;</td><td>1</td></tr><tr><td>&quot;CraigCrawfordMP&quot;</td><td>1</td></tr><tr><td>&quot;JGilbertMP&quot;</td><td>1</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;camerondickqld&quot;</td><td>107</td></tr><tr><td>&quot;mireelr&quot;</td><td>119</td></tr><tr><td>&quot;BrunkerMichael&quot;</td><td>172</td></tr><tr><td>&quot;MarkBaileyMP&quot;</td><td>311</td></tr><tr><td>&quot;AnnastaciaMP&quot;</td><td>315</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (79, 2)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ username        â”† text â”‚\n",
       "â”‚ ---             â”† ---  â”‚\n",
       "â”‚ str             â”† u32  â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•¡\n",
       "â”‚ meaghan_scanlon â”† 1    â”‚\n",
       "â”‚ Ros_Bates_MP    â”† 1    â”‚\n",
       "â”‚ Bennett4Burnett â”† 1    â”‚\n",
       "â”‚ CraigCrawfordMP â”† 1    â”‚\n",
       "â”‚ JGilbertMP      â”† 1    â”‚\n",
       "â”‚ â€¦               â”† â€¦    â”‚\n",
       "â”‚ camerondickqld  â”† 107  â”‚\n",
       "â”‚ mireelr         â”† 119  â”‚\n",
       "â”‚ BrunkerMichael  â”† 172  â”‚\n",
       "â”‚ MarkBaileyMP    â”† 311  â”‚\n",
       "â”‚ AnnastaciaMP    â”† 315  â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.group_by('username').agg([\n",
    "    pl.col('text').count(),]).sort('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c943ce7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tweets_df_AnnastaciaMP': {'0': 12,\n",
       "  '1': 16,\n",
       "  '10': 3,\n",
       "  '100': 1,\n",
       "  '1000': 2,\n",
       "  '100k': 1,\n",
       "  '100m': 1,\n",
       "  '1015': 0,\n",
       "  '10km': 0,\n",
       "  '10m': 0,\n",
       "  '10newsfirstqld': 0,\n",
       "  '10year': 2,\n",
       "  '11': 1,\n",
       "  '1100': 0,\n",
       "  '1139': 2,\n",
       "  '1154': 5,\n",
       "  '1155': 3,\n",
       "  '1156': 1,\n",
       "  '1157': 3,\n",
       "  '1159': 2,\n",
       "  '1160': 2,\n",
       "  '1163': 1,\n",
       "  '1164': 4,\n",
       "  '1165': 2,\n",
       "  '1167': 5,\n",
       "  '1169': 1,\n",
       "  '1171': 2,\n",
       "  '1172': 3,\n",
       "  '1184163': 1,\n",
       "  '1186578': 1,\n",
       "  '1188355': 1,\n",
       "  '1192435': 1,\n",
       "  '1198424': 1,\n",
       "  '12': 1,\n",
       "  '120': 1,\n",
       "  '1203184': 1,\n",
       "  '1207967': 1,\n",
       "  '120m': 0,\n",
       "  '1211609': 1,\n",
       "  '1214121': 1,\n",
       "  '1215772': 1,\n",
       "  '1219570': 1,\n",
       "  '1224099': 1,\n",
       "  '1229045': 1,\n",
       "  '1233236': 1,\n",
       "  '1236847': 1,\n",
       "  '1239418': 1,\n",
       "  '124': 0,\n",
       "  '1241130': 1,\n",
       "  '126': 1,\n",
       "  '126b': 0,\n",
       "  '13': 1,\n",
       "  '137': 0,\n",
       "  '13b': 1,\n",
       "  '14': 1,\n",
       "  '140': 4,\n",
       "  '14000': 0,\n",
       "  '15': 2,\n",
       "  '150000': 0,\n",
       "  '154': 0,\n",
       "  '15b': 0,\n",
       "  '16': 1,\n",
       "  '160': 1,\n",
       "  '16b': 0,\n",
       "  '16km': 0,\n",
       "  '17': 1,\n",
       "  '171': 1,\n",
       "  '17800': 1,\n",
       "  '19': 2,\n",
       "  '193': 1,\n",
       "  '193m': 1,\n",
       "  '1am': 1,\n",
       "  '1b': 1,\n",
       "  '1m': 0,\n",
       "  '1st': 1,\n",
       "  '2': 8,\n",
       "  '20': 1,\n",
       "  '200': 1,\n",
       "  '200m': 1,\n",
       "  '2015': 2,\n",
       "  '2016': 1,\n",
       "  '2017': 0,\n",
       "  '2018': 1,\n",
       "  '2020': 6,\n",
       "  '20202030': 1,\n",
       "  '2021': 4,\n",
       "  '2023': 3,\n",
       "  '2025': 4,\n",
       "  '2030': 0,\n",
       "  '208': 0,\n",
       "  '21': 1,\n",
       "  '21m': 0,\n",
       "  '22': 1,\n",
       "  '220': 1,\n",
       "  '22m': 0,\n",
       "  '23': 1,\n",
       "  '23000': 0,\n",
       "  '24': 2,\n",
       "  '25': 5,\n",
       "  '2500': 1,\n",
       "  '25b': 0,\n",
       "  '25s': 2,\n",
       "  '26': 1,\n",
       "  '263b': 0,\n",
       "  '2655': 0,\n",
       "  '26b': 0,\n",
       "  '27': 2,\n",
       "  '28': 1,\n",
       "  '28000': 0,\n",
       "  '29': 1,\n",
       "  '2nd': 0,\n",
       "  '2yr': 0,\n",
       "  '3': 4,\n",
       "  '30': 3,\n",
       "  '300': 3,\n",
       "  '30000': 3,\n",
       "  '300m': 0,\n",
       "  '30m': 1,\n",
       "  '31': 2,\n",
       "  '32000': 0,\n",
       "  '33': 3,\n",
       "  '33b': 0,\n",
       "  '33billionshort': 0,\n",
       "  '35000': 1,\n",
       "  '357': 1,\n",
       "  '37': 1,\n",
       "  '4': 6,\n",
       "  '40': 5,\n",
       "  '400000': 2,\n",
       "  '400m': 1,\n",
       "  '405': 1,\n",
       "  '40m': 1,\n",
       "  '430': 1,\n",
       "  '46': 0,\n",
       "  '481m': 0,\n",
       "  '5': 5,\n",
       "  '50': 3,\n",
       "  '50000': 1,\n",
       "  '500m': 3,\n",
       "  '55': 1,\n",
       "  '55000': 1,\n",
       "  '577000': 2,\n",
       "  '58m': 0,\n",
       "  '5918': 1,\n",
       "  '5b': 0,\n",
       "  '5pm': 0,\n",
       "  '6': 4,\n",
       "  '60': 2,\n",
       "  '60000': 1,\n",
       "  '60m': 1,\n",
       "  '6100': 0,\n",
       "  '6190': 2,\n",
       "  '6500': 2,\n",
       "  '68': 0,\n",
       "  '69': 0,\n",
       "  '695': 0,\n",
       "  '6km': 0,\n",
       "  '6pm': 4,\n",
       "  '700': 0,\n",
       "  '709': 1,\n",
       "  '709m': 0,\n",
       "  '716000': 1,\n",
       "  '7300': 1,\n",
       "  '74': 4,\n",
       "  '75': 2,\n",
       "  '760': 1,\n",
       "  '77': 1,\n",
       "  '78': 1,\n",
       "  '78b': 2,\n",
       "  '7news': 6,\n",
       "  '7newsaustralia': 0,\n",
       "  '7newsbrisbane': 5,\n",
       "  '7newsgoldcoast': 1,\n",
       "  '7newssc': 0,\n",
       "  '8': 0,\n",
       "  '80': 0,\n",
       "  '80m': 0,\n",
       "  '84': 0,\n",
       "  '86': 1,\n",
       "  '880': 1,\n",
       "  '90': 1,\n",
       "  '9475': 1,\n",
       "  '9am': 0,\n",
       "  '9news': 2,\n",
       "  '9newsqueensland': 2,\n",
       "  '9pm': 0,\n",
       "  'a': 128,\n",
       "  'aa': 0,\n",
       "  'aaronharpermp': 2,\n",
       "  'abc': 0,\n",
       "  'abcaustralia': 0,\n",
       "  'abcnews': 1,\n",
       "  'able': 6,\n",
       "  'aboard': 1,\n",
       "  'abolish': 0,\n",
       "  'aboriginal': 0,\n",
       "  'abortion': 0,\n",
       "  'about': 18,\n",
       "  'above': 2,\n",
       "  'abs': 0,\n",
       "  'absolutely': 1,\n",
       "  'absorbed': 1,\n",
       "  'abuse': 0,\n",
       "  'accepted': 0,\n",
       "  'access': 3,\n",
       "  'account': 0,\n",
       "  'accountability': 0,\n",
       "  'accountable': 0,\n",
       "  'accused': 0,\n",
       "  'achieve': 0,\n",
       "  'achieved': 1,\n",
       "  'achiever': 0,\n",
       "  'acquired': 4,\n",
       "  'across': 12,\n",
       "  'act': 2,\n",
       "  'acted': 1,\n",
       "  'acting': 0,\n",
       "  'action': 2,\n",
       "  'actions': 1,\n",
       "  'activate': 0,\n",
       "  'activating': 1,\n",
       "  'active': 18,\n",
       "  'activities': 1,\n",
       "  'activity': 1,\n",
       "  'actual': 0,\n",
       "  'actually': 0,\n",
       "  'acutely': 1,\n",
       "  'ad': 0,\n",
       "  'adaptation': 1,\n",
       "  'add': 0,\n",
       "  'additional': 3,\n",
       "  'additionally': 4,\n",
       "  'addressing': 1,\n",
       "  'adhere': 0,\n",
       "  'admin': 0,\n",
       "  'administer': 1,\n",
       "  'admiration': 0,\n",
       "  'admirer': 0,\n",
       "  'admires': 0,\n",
       "  'admit': 0,\n",
       "  'adopting': 1,\n",
       "  'adorable': 1,\n",
       "  'advice': 1,\n",
       "  'advocacy': 0,\n",
       "  'advocate': 0,\n",
       "  'advocating': 0,\n",
       "  'affected': 1,\n",
       "  'affirming': 0,\n",
       "  'afford': 2,\n",
       "  'afl': 2,\n",
       "  'aflfinals': 1,\n",
       "  'aflgf': 2,\n",
       "  'after': 8,\n",
       "  'afternoon': 2,\n",
       "  'again': 1,\n",
       "  'against': 0,\n",
       "  'age': 1,\n",
       "  'aged': 3,\n",
       "  'agforceqld': 1,\n",
       "  'ago': 1,\n",
       "  'agreement': 0,\n",
       "  'agriculture': 2,\n",
       "  'ah': 0,\n",
       "  'ahead': 1,\n",
       "  'ahhhhhh': 0,\n",
       "  'aides': 4,\n",
       "  'airportbug': 0,\n",
       "  'aka': 0,\n",
       "  'albomp': 0,\n",
       "  'algester': 3,\n",
       "  'alienarmy1': 3,\n",
       "  'all': 14,\n",
       "  'alleged': 0,\n",
       "  'allied': 1,\n",
       "  'allies': 0,\n",
       "  'allocated': 0,\n",
       "  'allow': 1,\n",
       "  'allowed': 4,\n",
       "  'almost': 3,\n",
       "  'alone': 0,\n",
       "  'along': 3,\n",
       "  'alp': 0,\n",
       "  'already': 9,\n",
       "  'also': 13,\n",
       "  'alternative': 1,\n",
       "  'alternatives': 2,\n",
       "  'always': 8,\n",
       "  'am': 1,\n",
       "  'amandacarlile5': 5,\n",
       "  'amazing': 2,\n",
       "  'ambitious': 0,\n",
       "  'ambos': 2,\n",
       "  'ambulance': 1,\n",
       "  'amendments': 0,\n",
       "  'america': 0,\n",
       "  'among': 1,\n",
       "  'amount': 0,\n",
       "  'amp': 6,\n",
       "  'an': 30,\n",
       "  'anagnorisis1234': 2,\n",
       "  'analysis': 0,\n",
       "  'and': 256,\n",
       "  'animal': 1,\n",
       "  'animals': 1,\n",
       "  'annarawlings': 0,\n",
       "  'annastacia': 2,\n",
       "  'annastaciamp': 15,\n",
       "  'annastaciampâ€™s': 0,\n",
       "  'annerleylabor': 0,\n",
       "  'annie': 1,\n",
       "  'anniversary': 0,\n",
       "  'announce': 2,\n",
       "  'announced': 4,\n",
       "  'announcement': 6,\n",
       "  'announcements': 1,\n",
       "  'announcing': 2,\n",
       "  'annual': 1,\n",
       "  'annually': 2,\n",
       "  'another': 8,\n",
       "  'answer': 1,\n",
       "  'answered': 1,\n",
       "  'anti': 0,\n",
       "  'antilabor': 0,\n",
       "  'antonygreenabc': 0,\n",
       "  'any': 2,\n",
       "  'anzac': 1,\n",
       "  'apology': 0,\n",
       "  'apostles': 1,\n",
       "  'apprenticeships': 1,\n",
       "  'appropriately': 0,\n",
       "  'approvals': 0,\n",
       "  'approved': 0,\n",
       "  'are': 23,\n",
       "  'area': 5,\n",
       "  'areas': 2,\n",
       "  'ariadne': 1,\n",
       "  'arithmetic': 0,\n",
       "  'arms': 1,\n",
       "  'around': 7,\n",
       "  'arrival': 0,\n",
       "  'arrived': 0,\n",
       "  'arterial': 1,\n",
       "  'artists': 1,\n",
       "  'as': 31,\n",
       "  'ashamed': 0,\n",
       "  'asked': 3,\n",
       "  'asking': 7,\n",
       "  'asks': 1,\n",
       "  'asphalt': 0,\n",
       "  'aspiring': 2,\n",
       "  'aspley': 2,\n",
       "  'assets': 3,\n",
       "  'assistance': 1,\n",
       "  'assistant': 0,\n",
       "  'assisted': 1,\n",
       "  'assume': 0,\n",
       "  'astounding': 0,\n",
       "  'at': 55,\n",
       "  'atm': 0,\n",
       "  'attention': 2,\n",
       "  'attitude': 1,\n",
       "  'attraction': 1,\n",
       "  'attractions': 1,\n",
       "  'attracts': 0,\n",
       "  'august': 1,\n",
       "  'aus': 0,\n",
       "  'auspol': 0,\n",
       "  'aussie': 1,\n",
       "  'australia': 3,\n",
       "  'australianlabor': 1,\n",
       "  'australians': 1,\n",
       "  'australiazoo': 10,\n",
       "  'australiaâ€™s': 1,\n",
       "  'ausvinyl': 1,\n",
       "  'autostadt': 0,\n",
       "  'available': 1,\n",
       "  'ave': 1,\n",
       "  'average': 2,\n",
       "  'avoided': 0,\n",
       "  'awake': 1,\n",
       "  'aware': 1,\n",
       "  'awayâ€': 0,\n",
       "  'ayrğŸ‘': 1,\n",
       "  'baby': 3,\n",
       "  'back': 13,\n",
       "  'backing': 4,\n",
       "  'backs': 1,\n",
       "  'backwards': 2,\n",
       "  'bag': 1,\n",
       "  'balanced': 0,\n",
       "  'ban': 2,\n",
       "  'banknotes': 0,\n",
       "  'banned': 0,\n",
       "  'barrier': 6,\n",
       "  'bart': 3,\n",
       "  'bartholomew': 0,\n",
       "  'bartâ€™s': 1,\n",
       "  'based': 0,\n",
       "  'basketball': 1,\n",
       "  'bay': 0,\n",
       "  'bayside': 1,\n",
       "  'bc': 0,\n",
       "  'bcc': 0,\n",
       "  'be': 23,\n",
       "  'beach': 1,\n",
       "  'beaches': 1,\n",
       "  'beats': 1,\n",
       "  'beattie': 0,\n",
       "  'beautiful': 4,\n",
       "  'beauty': 1,\n",
       "  'became': 0,\n",
       "  'because': 15,\n",
       "  'beckmanns': 0,\n",
       "  'become': 1,\n",
       "  'been': 16,\n",
       "  'beenleigh': 0,\n",
       "  'beenleighredland': 0,\n",
       "  'before': 9,\n",
       "  'began': 1,\n",
       "  'begun': 0,\n",
       "  'behaviour': 1,\n",
       "  'behind': 1,\n",
       "  'beholden': 0,\n",
       "  'being': 1,\n",
       "  'believe': 3,\n",
       "  'believes': 2,\n",
       "  'bellzwebster': 4,\n",
       "  'below': 4,\n",
       "  'benefit': 1,\n",
       "  'benefits': 1,\n",
       "  'benjaminwt': 1,\n",
       "  'berkman': 0,\n",
       "  'berths': 1,\n",
       "  'best': 12,\n",
       "  'better': 3,\n",
       "  'between': 3,\n",
       "  'bicyclemackay': 0,\n",
       "  'bicycleqld': 0,\n",
       "  'big': 5,\n",
       "  'biggest': 4,\n",
       "  'bikeway': 0,\n",
       "  'bikeways': 0,\n",
       "  'bill': 0,\n",
       "  'billion': 11,\n",
       "  'billionaire': 0,\n",
       "  'billions': 0,\n",
       "  'bills': 1,\n",
       "  'billypinker': 3,\n",
       "  'bindiirwin': 1,\n",
       "  'binned': 0,\n",
       "  'bioenergy': 1,\n",
       "  'biofutures': 2,\n",
       "  'biologist': 1,\n",
       "  'birds': 2,\n",
       "  'bitch': 0,\n",
       "  'bitumen': 0,\n",
       "  'black': 1,\n",
       "  'blame': 0,\n",
       "  'blitzing': 1,\n",
       "  'blue': 0,\n",
       "  'bluey': 1,\n",
       "  'blunder': 0,\n",
       "  'bob': 1,\n",
       "  'boilermaker': 1,\n",
       "  'bold': 0,\n",
       "  'bonney': 0,\n",
       "  'booking': 1,\n",
       "  'boom': 0,\n",
       "  'boost': 1,\n",
       "  'booth': 5,\n",
       "  'booths': 2,\n",
       "  'borbidge': 0,\n",
       "  'border': 2,\n",
       "  'borders': 2,\n",
       "  'borrow': 0,\n",
       "  'borrowings': 2,\n",
       "  'boss': 0,\n",
       "  'both': 2,\n",
       "  'bounce': 1,\n",
       "  'bowen': 2,\n",
       "  'bowens': 1,\n",
       "  'bowling': 1,\n",
       "  'boys': 0,\n",
       "  'bradfield': 0,\n",
       "  'bragging': 0,\n",
       "  'branch': 0,\n",
       "  'brazen00': 0,\n",
       "  'breaches': 0,\n",
       "  'breaking': 4,\n",
       "  'breenie9': 0,\n",
       "  'breeze': 0,\n",
       "  'brew': 1,\n",
       "  'bribie': 1,\n",
       "  'brichardson1964': 2,\n",
       "  'bridge': 2,\n",
       "  'bridgettejorja': 2,\n",
       "  'brilliant': 1,\n",
       "  'bring': 1,\n",
       "  'bris': 0,\n",
       "  'brisbane': 1,\n",
       "  'brisbanebroncos': 2,\n",
       "  'brisbanelions': 1,\n",
       "  'brisbanewestbug': 0,\n",
       "  'brisbaneâ€™s': 1,\n",
       "  'broadbeach': 1,\n",
       "  'broadwater': 0,\n",
       "  'brothers': 1,\n",
       "  'brought': 1,\n",
       "  'bruce': 19,\n",
       "  'brutal': 0,\n",
       "  'bryan': 0,\n",
       "  'bub': 1,\n",
       "  'buckle': 1,\n",
       "  'buckled': 0,\n",
       "  'budget': 5,\n",
       "  'budgets': 2,\n",
       "  'bugnorth': 0,\n",
       "  'build': 8,\n",
       "  'building': 5,\n",
       "  'builds': 1,\n",
       "  'built': 1,\n",
       "  'bull': 0,\n",
       "  'bundaberg': 1,\n",
       "  'bundabergdrinks': 1,\n",
       "  'bundamba': 1,\n",
       "  'bundlesnl': 0,\n",
       "  'burleigh': 2,\n",
       "  'bus': 1,\n",
       "  'buses': 0,\n",
       "  'bushwalk': 1,\n",
       "  'busiest': 1,\n",
       "  'business': 12,\n",
       "  'businesses': 14,\n",
       "  'bust': 1,\n",
       "  'busted': 0,\n",
       "  'busway': 1,\n",
       "  'busâ€™': 0,\n",
       "  'but': 8,\n",
       "  'buy': 1,\n",
       "  'by': 25,\n",
       "  'cableway': 1,\n",
       "  'caesar': 1,\n",
       "  'cafe': 1,\n",
       "  'cafÃ©': 1,\n",
       "  'cairns': 7,\n",
       "  'cajeffrey14': 0,\n",
       "  'calamvale': 1,\n",
       "  'call': 1,\n",
       "  'called': 0,\n",
       "  'calls': 0,\n",
       "  'caloundra': 1,\n",
       "  'came': 1,\n",
       "  'cameras': 0,\n",
       "  'camerondickqld': 4,\n",
       "  'campaign': 3,\n",
       "  'campaigners': 0,\n",
       "  'campaigning': 0,\n",
       "  'campaignlife': 1,\n",
       "  'campaignâ€': 0,\n",
       "  'campaignğŸï¸ğŸ”¥': 1,\n",
       "  'campbell': 1,\n",
       "  'campbellnewman': 0,\n",
       "  'campbellnewmanâ€™s': 0,\n",
       "  'campradt': 0,\n",
       "  'camsmith9': 1,\n",
       "  'can': 18,\n",
       "  'canada': 0,\n",
       "  'canberra': 0,\n",
       "  'candidate': 2,\n",
       "  'candidates': 0,\n",
       "  'cannot': 1,\n",
       "  'cant': 1,\n",
       "  'canâ€™t': 13,\n",
       "  'cap': 0,\n",
       "  'capacity': 1,\n",
       "  'car': 0,\n",
       "  'card': 0,\n",
       "  'cards': 1,\n",
       "  'care': 8,\n",
       "  'careers': 1,\n",
       "  'cares': 1,\n",
       "  'caretaker': 1,\n",
       "  'carl': 1,\n",
       "  'carole': 1,\n",
       "  'carrie': 1,\n",
       "  'case': 6,\n",
       "  'cases': 73,\n",
       "  'cassowary': 3,\n",
       "  'cast': 1,\n",
       "  'casualty': 1,\n",
       "  'catch': 3,\n",
       "  'catching': 2,\n",
       "  'catedempsey': 2,\n",
       "  'caught': 0,\n",
       "  'cbdbug': 0,\n",
       "  'cdstrunk': 1,\n",
       "  'cent': 2,\n",
       "  'centre': 3,\n",
       "  'centrepiece': 0,\n",
       "  'centres': 2,\n",
       "  'centuryyuasa': 2,\n",
       "  'chains': 0,\n",
       "  'challenges': 1,\n",
       "  'chance': 1,\n",
       "  'change': 4,\n",
       "  'changed': 0,\n",
       "  'changer': 1,\n",
       "  'changerooms': 1,\n",
       "  'channel': 1,\n",
       "  'chaos': 2,\n",
       "  'chaosqldvotes': 1,\n",
       "  'chaotic': 0,\n",
       "  'chapter': 1,\n",
       "  'charge': 0,\n",
       "  'charged': 0,\n",
       "  'charismullenmp': 1,\n",
       "  'charters': 3,\n",
       "  'chats': 0,\n",
       "  'chatted': 0,\n",
       "  'chatting': 1,\n",
       "  'check': 1,\n",
       "  'checking': 1,\n",
       "  'chief': 1,\n",
       "  'child': 4,\n",
       "  'children': 3,\n",
       "  'chill': 1,\n",
       "  'choice': 9,\n",
       "  'choose': 0,\n",
       "  'chris4coomera': 2,\n",
       "  'christian': 0,\n",
       "  'church': 1,\n",
       "  'ciaraejones': 0,\n",
       "  'city': 1,\n",
       "  'civil': 0,\n",
       "  'ck': 0,\n",
       "  'claims': 0,\n",
       "  'clarebarnes10': 0,\n",
       "  'clean': 0,\n",
       "  'cleannrgcouncil': 0,\n",
       "  'clear': 11,\n",
       "  'clearing': 1,\n",
       "  'clearly': 0,\n",
       "  'clevelandredland': 0,\n",
       "  'climate': 1,\n",
       "  'clive': 0,\n",
       "  'clivefpalmer': 2,\n",
       "  'cliveâ€™s': 0,\n",
       "  'close': 4,\n",
       "  'closed': 1,\n",
       "  'closer': 1,\n",
       "  'cloth': 0,\n",
       "  'club': 2,\n",
       "  'clubhouse': 2,\n",
       "  'clubs': 2,\n",
       "  'coal': 0,\n",
       "  'coalition': 0,\n",
       "  'coast': 13,\n",
       "  'coastâ€™s': 1,\n",
       "  'coffee': 1,\n",
       "  'cold': 0,\n",
       "  'colincaudell': 0,\n",
       "  'collapsed': 0,\n",
       "  'collar': 0,\n",
       "  'colleague': 0,\n",
       "  'college': 2,\n",
       "  'colourful': 1,\n",
       "  'come': 8,\n",
       "  'comes': 1,\n",
       "  'coming': 4,\n",
       "  'comments': 0,\n",
       "  'commissioned': 1,\n",
       "  'commitment': 2,\n",
       "  'commitments': 5,\n",
       "  'committed': 10,\n",
       "  'committing': 2,\n",
       "  'commonheather': 3,\n",
       "  'communities': 2,\n",
       "  'community': 10,\n",
       "  'company': 5,\n",
       "  'complete': 3,\n",
       "  'completed': 0,\n",
       "  'compliant': 0,\n",
       "  'comprehensive': 1,\n",
       "  'compulsory': 0,\n",
       "  'concerns': 1,\n",
       "  'concessional': 1,\n",
       "  'conducted': 17,\n",
       "  'conference': 3,\n",
       "  'conferences': 1,\n",
       "  'confessed': 0,\n",
       "  'confirmed': 34,\n",
       "  'confirms': 0,\n",
       "  'congestion': 1,\n",
       "  'congrats': 0,\n",
       "  'congratulations': 2,\n",
       "  'connector': 1,\n",
       "  'conscious': 0,\n",
       "  'conservation': 2,\n",
       "  'considering': 1,\n",
       "  'constituents': 0,\n",
       "  'construction': 4,\n",
       "  'consultations': 1,\n",
       "  'contact': 0,\n",
       "  'contain': 1,\n",
       "  'container': 1,\n",
       "  'containers': 1,\n",
       "  'content': 0,\n",
       "  'contested': 0,\n",
       "  'continue': 23,\n",
       "  'continues': 6,\n",
       "  'contracting': 0,\n",
       "  'contractors': 1,\n",
       "  'contradicted': 0,\n",
       "  'contrast': 1,\n",
       "  'conventional': 0,\n",
       "  'conway': 1,\n",
       "  'cookdâ€™aguilar': 0,\n",
       "  'coolangatta': 2,\n",
       "  'coomera': 2,\n",
       "  'corinda': 1,\n",
       "  'corner': 1,\n",
       "  'coronavirus': 21,\n",
       "  'cost': 1,\n",
       "  'costings': 1,\n",
       "  'costs': 2,\n",
       "  'costumes': 1,\n",
       "  'could': 6,\n",
       "  'couldnâ€™t': 2,\n",
       "  'councillors': 1,\n",
       "  'councils': 1,\n",
       "  'count': 0,\n",
       "  'counting': 1,\n",
       "  'countries': 2,\n",
       "  'country': 1,\n",
       "  'couple': 1,\n",
       "  'courage': 1,\n",
       "  'courageous': 0,\n",
       "  'couriermail': 0,\n",
       "  'course': 3,\n",
       "  'courses': 1,\n",
       "  'courtesy': 1,\n",
       "  'cover': 1,\n",
       "  'covid': 7,\n",
       "  'covid19': 49,\n",
       "  'covid19au': 7,\n",
       "  'cowboys': 1,\n",
       "  'cowboysleagues': 1,\n",
       "  'coxyjindas': 0,\n",
       "  'cq': 0,\n",
       "  'craigcrawfordmp': 6,\n",
       "  'crashing': 1,\n",
       "  'create': 12,\n",
       "  'created': 3,\n",
       "  'creating': 9,\n",
       "  'creation': 1,\n",
       "  'credit': 0,\n",
       "  'credits': 1,\n",
       "  'crew': 1,\n",
       "  'crisis': 1,\n",
       "  'critical': 0,\n",
       "  'critically': 1,\n",
       "  'criticising': 0,\n",
       "  'critters': 1,\n",
       "  'crleswalker': 2,\n",
       "  'crops': 1,\n",
       "  'cross': 0,\n",
       "  'crownofthorns': 1,\n",
       "  'cruise': 0,\n",
       "  'cruises': 0,\n",
       "  'crumbed': 0,\n",
       "  'crunching': 0,\n",
       "  'cuddling': 1,\n",
       "  'cultural': 1,\n",
       "  'culture': 0,\n",
       "  'cup': 1,\n",
       "  'current': 6,\n",
       "  'currumbin': 2,\n",
       "  'curtispittmp': 0,\n",
       "  'cut': 3,\n",
       "  'cuts': 5,\n",
       "  'cutting': 1,\n",
       "  'cwhfau': 3,\n",
       "  'cwslive': 1,\n",
       "  'cycleway': 0,\n",
       "  'cycleways': 0,\n",
       "  'daily': 0,\n",
       "  'daisy': 1,\n",
       "  'daleypearson': 1,\n",
       "  'dam': 1,\n",
       "  'damage': 0,\n",
       "  'dams': 0,\n",
       "  'dancing': 1,\n",
       "  'data': 0,\n",
       "  'date': 0,\n",
       "  'davethebass': 0,\n",
       "  'david': 0,\n",
       "  'davidcrisafulli': 0,\n",
       "  'dawe': 0,\n",
       "  'day': 15,\n",
       "  'days': 4,\n",
       "  'de': 1,\n",
       "  'deal': 0,\n",
       "  'death': 0,\n",
       "  'deaths': 1,\n",
       "  'deb': 3,\n",
       "  'debate': 2,\n",
       "  'debfrecklington': 1,\n",
       "  'debfrecklingtons': 0,\n",
       "  'debfrecklingtonâ€™s': 0,\n",
       "  'debfrecklington\\u2069': 0,\n",
       "  'debilitating': 1,\n",
       "  'debritz': 0,\n",
       "  'debs': 0,\n",
       "  'debâ€™s': 1,\n",
       "  'deceitful': 0,\n",
       "  'december': 2,\n",
       "  'deciders': 1,\n",
       "  'decimated': 0,\n",
       "  'decision': 0,\n",
       "  'decisions': 3,\n",
       "  'decisive': 2,\n",
       "  'decriminalisation': 0,\n",
       "  'dee': 0,\n",
       "  'deeem777': 3,\n",
       "  'deemadigan': 0,\n",
       "  'deeper': 0,\n",
       "  'deeply': 0,\n",
       "  'defeat': 0,\n",
       "  'delicious': 1,\n",
       "  'delish': 1,\n",
       "  'deliver': 8,\n",
       "  'delivered': 4,\n",
       "  'delivering': 6,\n",
       "  'delivers': 0,\n",
       "  'delivery': 2,\n",
       "  'delma': 1,\n",
       "  'deluge': 0,\n",
       "  'democracy': 3,\n",
       "  'democracysausage': 2,\n",
       "  'denied': 0,\n",
       "  'denouncing': 0,\n",
       "  'department': 0,\n",
       "  'deputy': 0,\n",
       "  'deserve': 2,\n",
       "  'deserves': 2,\n",
       "  'designing': 0,\n",
       "  'despite': 0,\n",
       "  'destination': 1,\n",
       "  'destinations': 1,\n",
       "  'destined': 1,\n",
       "  'destroying': 1,\n",
       "  'detail': 1,\n",
       "  'details': 1,\n",
       "  'detected': 4,\n",
       "  'detection': 0,\n",
       "  'detour': 3,\n",
       "  'devastated': 0,\n",
       "  'develop': 1,\n",
       "  'developed': 1,\n",
       "  'developers': 0,\n",
       "  'developing': 1,\n",
       "  'development': 1,\n",
       "  'devils': 1,\n",
       "  'diabolical': 0,\n",
       "  'did': 4,\n",
       "  'didnâ€™t': 0,\n",
       "  'died': 17,\n",
       "  'dipping': 1,\n",
       "  'direct': 1,\n",
       "  'direction': 1,\n",
       "  'directly': 1,\n",
       "  'director': 0,\n",
       "  'disability': 0,\n",
       "  'discount': 0,\n",
       "  'discovered': 1,\n",
       "  'disgruntled': 0,\n",
       "  'dishonesty': 0,\n",
       "  'disperse': 0,\n",
       "  'distance': 0,\n",
       "  'distancing': 0,\n",
       "  'dna': 0,\n",
       "  'do': 11,\n",
       "  'dockside': 0,\n",
       "  'doctor': 0,\n",
       "  'doctors': 6,\n",
       "  'document': 1,\n",
       "  'dodginess': 0,\n",
       "  'dodging': 0,\n",
       "  'does': 0,\n",
       "  'doesnt': 0,\n",
       "  'doesnâ€™t': 0,\n",
       "  'doing': 4,\n",
       "  'dollar': 0,\n",
       "  'don': 0,\n",
       "  'donations': 0,\n",
       "  'donbrownmp': 0,\n",
       "  'done': 5,\n",
       "  'dont': 0,\n",
       "  'dontriskdeb': 0,\n",
       "  'donâ€™t': 1,\n",
       "  'doors': 1,\n",
       "  'doorstep': 1,\n",
       "  'double': 0,\n",
       "  'down': 7,\n",
       "  'downgraded': 0,\n",
       "  'downgrades': 0,\n",
       "  'downsdawson': 0,\n",
       "  'dozens': 1,\n",
       "  'dr': 0,\n",
       "  'dranthonylynham': 1,\n",
       "  'draw': 1,\n",
       "  'drawings': 1,\n",
       "  'draws': 0,\n",
       "  'drcrowanmp': 0,\n",
       "  'dress': 1,\n",
       "  'drinking': 2,\n",
       "  'drive': 1,\n",
       "  'driven': 0,\n",
       "  'drivers': 0,\n",
       "  'driving': 1,\n",
       "  'dropped': 2,\n",
       "  'drops': 0,\n",
       "  'due': 1,\n",
       "  'dump': 0,\n",
       "  'dumped': 0,\n",
       "  'duncan': 0,\n",
       "  'duncanpeggmp': 0,\n",
       "  'duplic': 0,\n",
       "  'duplicate': 0,\n",
       "  'duplicating': 0,\n",
       "  'duplication': 1,\n",
       "  'durack': 1,\n",
       "  'during': 6,\n",
       "  'dusted': 1,\n",
       "  'duties': 0,\n",
       "  'dying': 1,\n",
       "  'dâ€™aguilar': 0,\n",
       "  'each': 4,\n",
       "  'ear': 1,\n",
       "  'earlier': 0,\n",
       "  'early': 3,\n",
       "  'earthmoving': 1,\n",
       "  'ease': 1,\n",
       "  'easily': 0,\n",
       "  'easing': 1,\n",
       "  'east': 2,\n",
       "  'eastbug': 0,\n",
       "  'easy': 2,\n",
       "  'economic': 47,\n",
       "  'economies': 1,\n",
       "  'economy': 11,\n",
       "  'edmonton': 0,\n",
       "  'education': 13,\n",
       "  'effective': 0,\n",
       "  'effects': 2,\n",
       "  'effort': 1,\n",
       "  'eight': 1,\n",
       "  'elected': 1,\n",
       "  'election': 14,\n",
       "  'electionâ€™s': 0,\n",
       "  'electorate': 4,\n",
       "  'electric': 0,\n",
       "  'electrical': 1,\n",
       "  'electricity': 1,\n",
       "  'electrivenet': 0,\n",
       "  'elliotjstein': 0,\n",
       "  'embarrassing': 0,\n",
       "  'embarrassingly': 0,\n",
       "  'embarrassment': 0,\n",
       "  'emeleus': 0,\n",
       "  'emerges': 0,\n",
       "  'emissions': 2,\n",
       "  'employ': 0,\n",
       "  'employed': 2,\n",
       "  'employing': 1,\n",
       "  'employment': 2,\n",
       "  'empowering': 1,\n",
       "  'empty': 0,\n",
       "  'encouraging': 1,\n",
       "  'end': 2,\n",
       "  'endangered': 0,\n",
       "  'endorsed': 0,\n",
       "  'energy': 3,\n",
       "  'engine': 2,\n",
       "  'englandâ€™s': 1,\n",
       "  'enough': 2,\n",
       "  'ensure': 3,\n",
       "  'enter': 2,\n",
       "  'entire': 1,\n",
       "  'entirely': 0,\n",
       "  'environment': 6,\n",
       "  'environmental': 3,\n",
       "  'environmentally': 0,\n",
       "  'episode': 1,\n",
       "  'equating': 0,\n",
       "  'equipment': 2,\n",
       "  'equivalent': 1,\n",
       "  'ergonenergy': 0,\n",
       "  'ersqueensland': 0,\n",
       "  'especially': 0,\n",
       "  ...},\n",
       " 'tweets_df_MarkBaileyMP': {'0': 5,\n",
       "  '1': 8,\n",
       "  '10': 0,\n",
       "  '100': 3,\n",
       "  '1000': 1,\n",
       "  '100k': 0,\n",
       "  '100m': 1,\n",
       "  '1015': 1,\n",
       "  '10km': 1,\n",
       "  '10m': 2,\n",
       "  '10newsfirstqld': 6,\n",
       "  '10year': 0,\n",
       "  '11': 0,\n",
       "  '1100': 1,\n",
       "  '1139': 0,\n",
       "  '1154': 1,\n",
       "  '1155': 2,\n",
       "  '1156': 0,\n",
       "  '1157': 0,\n",
       "  '1159': 0,\n",
       "  '1160': 1,\n",
       "  '1163': 1,\n",
       "  '1164': 1,\n",
       "  '1165': 1,\n",
       "  '1167': 1,\n",
       "  '1169': 0,\n",
       "  '1171': 0,\n",
       "  '1172': 2,\n",
       "  '1184163': 1,\n",
       "  '1186578': 0,\n",
       "  '1188355': 0,\n",
       "  '1192435': 0,\n",
       "  '1198424': 0,\n",
       "  '12': 0,\n",
       "  '120': 0,\n",
       "  '1203184': 1,\n",
       "  '1207967': 0,\n",
       "  '120m': 1,\n",
       "  '1211609': 0,\n",
       "  '1214121': 1,\n",
       "  '1215772': 0,\n",
       "  '1219570': 0,\n",
       "  '1224099': 0,\n",
       "  '1229045': 0,\n",
       "  '1233236': 0,\n",
       "  '1236847': 0,\n",
       "  '1239418': 1,\n",
       "  '124': 2,\n",
       "  '1241130': 1,\n",
       "  '126': 1,\n",
       "  '126b': 1,\n",
       "  '13': 0,\n",
       "  '137': 1,\n",
       "  '13b': 0,\n",
       "  '14': 0,\n",
       "  '140': 2,\n",
       "  '14000': 1,\n",
       "  '15': 2,\n",
       "  '150000': 1,\n",
       "  '154': 1,\n",
       "  '15b': 1,\n",
       "  '16': 2,\n",
       "  '160': 0,\n",
       "  '16b': 3,\n",
       "  '16km': 1,\n",
       "  '17': 1,\n",
       "  '171': 0,\n",
       "  '17800': 0,\n",
       "  '19': 1,\n",
       "  '193': 1,\n",
       "  '193m': 0,\n",
       "  '1am': 0,\n",
       "  '1b': 0,\n",
       "  '1m': 1,\n",
       "  '1st': 1,\n",
       "  '2': 5,\n",
       "  '20': 2,\n",
       "  '200': 1,\n",
       "  '200m': 0,\n",
       "  '2015': 0,\n",
       "  '2016': 0,\n",
       "  '2017': 3,\n",
       "  '2018': 2,\n",
       "  '2020': 2,\n",
       "  '20202030': 0,\n",
       "  '2021': 0,\n",
       "  '2023': 0,\n",
       "  '2025': 0,\n",
       "  '2030': 4,\n",
       "  '208': 1,\n",
       "  '21': 1,\n",
       "  '21m': 1,\n",
       "  '22': 1,\n",
       "  '220': 1,\n",
       "  '22m': 1,\n",
       "  '23': 0,\n",
       "  '23000': 1,\n",
       "  '24': 1,\n",
       "  '25': 3,\n",
       "  '2500': 0,\n",
       "  '25b': 1,\n",
       "  '25s': 0,\n",
       "  '26': 2,\n",
       "  '263b': 1,\n",
       "  '2655': 1,\n",
       "  '26b': 1,\n",
       "  '27': 1,\n",
       "  '28': 0,\n",
       "  '28000': 1,\n",
       "  '29': 1,\n",
       "  '2nd': 12,\n",
       "  '2yr': 1,\n",
       "  '3': 3,\n",
       "  '30': 0,\n",
       "  '300': 2,\n",
       "  '30000': 4,\n",
       "  '300m': 1,\n",
       "  '30m': 0,\n",
       "  '31': 0,\n",
       "  '32000': 1,\n",
       "  '33': 3,\n",
       "  '33b': 1,\n",
       "  '33billionshort': 1,\n",
       "  '35000': 0,\n",
       "  '357': 1,\n",
       "  '37': 1,\n",
       "  '4': 6,\n",
       "  '40': 1,\n",
       "  '400000': 0,\n",
       "  '400m': 0,\n",
       "  '405': 0,\n",
       "  '40m': 0,\n",
       "  '430': 0,\n",
       "  '46': 1,\n",
       "  '481m': 1,\n",
       "  '5': 6,\n",
       "  '50': 5,\n",
       "  '50000': 0,\n",
       "  '500m': 0,\n",
       "  '55': 0,\n",
       "  '55000': 0,\n",
       "  '577000': 0,\n",
       "  '58m': 1,\n",
       "  '5918': 1,\n",
       "  '5b': 1,\n",
       "  '5pm': 1,\n",
       "  '6': 5,\n",
       "  '60': 1,\n",
       "  '60000': 1,\n",
       "  '60m': 1,\n",
       "  '6100': 1,\n",
       "  '6190': 0,\n",
       "  '6500': 1,\n",
       "  '68': 2,\n",
       "  '69': 1,\n",
       "  '695': 1,\n",
       "  '6km': 3,\n",
       "  '6pm': 1,\n",
       "  '700': 1,\n",
       "  '709': 0,\n",
       "  '709m': 1,\n",
       "  '716000': 0,\n",
       "  '7300': 0,\n",
       "  '74': 0,\n",
       "  '75': 3,\n",
       "  '760': 0,\n",
       "  '77': 0,\n",
       "  '78': 0,\n",
       "  '78b': 0,\n",
       "  '7news': 3,\n",
       "  '7newsaustralia': 2,\n",
       "  '7newsbrisbane': 3,\n",
       "  '7newsgoldcoast': 0,\n",
       "  '7newssc': 1,\n",
       "  '8': 2,\n",
       "  '80': 1,\n",
       "  '80m': 1,\n",
       "  '84': 1,\n",
       "  '86': 0,\n",
       "  '880': 0,\n",
       "  '90': 0,\n",
       "  '9475': 0,\n",
       "  '9am': 1,\n",
       "  '9news': 1,\n",
       "  '9newsqueensland': 1,\n",
       "  '9pm': 1,\n",
       "  'a': 118,\n",
       "  'aa': 1,\n",
       "  'aaronharpermp': 0,\n",
       "  'abc': 1,\n",
       "  'abcaustralia': 1,\n",
       "  'abcnews': 1,\n",
       "  'able': 3,\n",
       "  'aboard': 0,\n",
       "  'abolish': 1,\n",
       "  'aboriginal': 1,\n",
       "  'abortion': 2,\n",
       "  'about': 20,\n",
       "  'above': 1,\n",
       "  'abs': 1,\n",
       "  'absolutely': 0,\n",
       "  'absorbed': 0,\n",
       "  'abuse': 1,\n",
       "  'accepted': 1,\n",
       "  'access': 2,\n",
       "  'account': 1,\n",
       "  'accountability': 1,\n",
       "  'accountable': 1,\n",
       "  'accused': 1,\n",
       "  'achieve': 1,\n",
       "  'achieved': 1,\n",
       "  'achiever': 1,\n",
       "  'acquired': 0,\n",
       "  'across': 4,\n",
       "  'act': 2,\n",
       "  'acted': 0,\n",
       "  'acting': 1,\n",
       "  'action': 1,\n",
       "  'actions': 1,\n",
       "  'activate': 1,\n",
       "  'activating': 1,\n",
       "  'active': 6,\n",
       "  'activities': 0,\n",
       "  'activity': 1,\n",
       "  'actual': 2,\n",
       "  'actually': 3,\n",
       "  'acutely': 0,\n",
       "  'ad': 1,\n",
       "  'adaptation': 0,\n",
       "  'add': 2,\n",
       "  'additional': 0,\n",
       "  'additionally': 0,\n",
       "  'addressing': 0,\n",
       "  'adhere': 1,\n",
       "  'admin': 2,\n",
       "  'administer': 0,\n",
       "  'admiration': 1,\n",
       "  'admirer': 2,\n",
       "  'admires': 1,\n",
       "  'admit': 3,\n",
       "  'adopting': 1,\n",
       "  'adorable': 0,\n",
       "  'advice': 7,\n",
       "  'advocacy': 1,\n",
       "  'advocate': 1,\n",
       "  'advocating': 2,\n",
       "  'affected': 0,\n",
       "  'affirming': 1,\n",
       "  'afford': 0,\n",
       "  'afl': 1,\n",
       "  'aflfinals': 0,\n",
       "  'aflgf': 0,\n",
       "  'after': 9,\n",
       "  'afternoon': 0,\n",
       "  'again': 8,\n",
       "  'against': 2,\n",
       "  'age': 0,\n",
       "  'aged': 1,\n",
       "  'agforceqld': 0,\n",
       "  'ago': 6,\n",
       "  'agreement': 2,\n",
       "  'agriculture': 0,\n",
       "  'ah': 1,\n",
       "  'ahead': 2,\n",
       "  'ahhhhhh': 1,\n",
       "  'aides': 1,\n",
       "  'airportbug': 3,\n",
       "  'aka': 1,\n",
       "  'albomp': 2,\n",
       "  'algester': 0,\n",
       "  'alienarmy1': 0,\n",
       "  'all': 15,\n",
       "  'alleged': 1,\n",
       "  'allied': 0,\n",
       "  'allies': 1,\n",
       "  'allocated': 2,\n",
       "  'allow': 1,\n",
       "  'allowed': 0,\n",
       "  'almost': 1,\n",
       "  'alone': 2,\n",
       "  'along': 2,\n",
       "  'alp': 1,\n",
       "  'already': 5,\n",
       "  'also': 4,\n",
       "  'alternative': 2,\n",
       "  'alternatives': 0,\n",
       "  'always': 6,\n",
       "  'am': 1,\n",
       "  'amandacarlile5': 0,\n",
       "  'amazing': 0,\n",
       "  'ambitious': 1,\n",
       "  'ambos': 0,\n",
       "  'ambulance': 0,\n",
       "  'amendments': 4,\n",
       "  'america': 1,\n",
       "  'among': 0,\n",
       "  'amount': 1,\n",
       "  'amp': 147,\n",
       "  'an': 22,\n",
       "  'anagnorisis1234': 0,\n",
       "  'analysis': 1,\n",
       "  'and': 75,\n",
       "  'animal': 0,\n",
       "  'animals': 0,\n",
       "  'annarawlings': 1,\n",
       "  'annastacia': 2,\n",
       "  'annastaciamp': 66,\n",
       "  'annastaciampâ€™s': 1,\n",
       "  'annerleylabor': 2,\n",
       "  'annie': 0,\n",
       "  'anniversary': 1,\n",
       "  'announce': 1,\n",
       "  'announced': 6,\n",
       "  'announcement': 5,\n",
       "  'announcements': 0,\n",
       "  'announcing': 0,\n",
       "  'annual': 0,\n",
       "  'annually': 0,\n",
       "  'another': 8,\n",
       "  'answer': 2,\n",
       "  'answered': 0,\n",
       "  'anti': 2,\n",
       "  'antilabor': 1,\n",
       "  'antonygreenabc': 1,\n",
       "  'any': 3,\n",
       "  'anzac': 0,\n",
       "  'apology': 2,\n",
       "  'apostles': 0,\n",
       "  'apprenticeships': 0,\n",
       "  'appropriately': 2,\n",
       "  'approvals': 1,\n",
       "  'approved': 2,\n",
       "  'are': 33,\n",
       "  'area': 0,\n",
       "  'areas': 0,\n",
       "  'ariadne': 0,\n",
       "  'arithmetic': 1,\n",
       "  'arms': 0,\n",
       "  'around': 3,\n",
       "  'arrival': 1,\n",
       "  'arrived': 1,\n",
       "  'arterial': 1,\n",
       "  'artists': 0,\n",
       "  'as': 26,\n",
       "  'ashamed': 1,\n",
       "  'asked': 1,\n",
       "  'asking': 1,\n",
       "  'asks': 0,\n",
       "  'asphalt': 1,\n",
       "  'aspiring': 0,\n",
       "  'aspley': 0,\n",
       "  'assets': 1,\n",
       "  'assistance': 0,\n",
       "  'assistant': 9,\n",
       "  'assisted': 4,\n",
       "  'assume': 1,\n",
       "  'astounding': 1,\n",
       "  'at': 27,\n",
       "  'atm': 1,\n",
       "  'attention': 0,\n",
       "  'attitude': 0,\n",
       "  'attraction': 0,\n",
       "  'attractions': 0,\n",
       "  'attracts': 1,\n",
       "  'august': 0,\n",
       "  'aus': 1,\n",
       "  'auspol': 3,\n",
       "  'aussie': 0,\n",
       "  'australia': 4,\n",
       "  'australianlabor': 0,\n",
       "  'australians': 0,\n",
       "  'australiazoo': 0,\n",
       "  'australiaâ€™s': 0,\n",
       "  'ausvinyl': 0,\n",
       "  'autostadt': 1,\n",
       "  'available': 1,\n",
       "  'ave': 0,\n",
       "  'average': 0,\n",
       "  'avoided': 1,\n",
       "  'awake': 0,\n",
       "  'aware': 0,\n",
       "  'awayâ€': 1,\n",
       "  'ayrğŸ‘': 0,\n",
       "  'baby': 0,\n",
       "  'back': 8,\n",
       "  'backing': 3,\n",
       "  'backs': 3,\n",
       "  'backwards': 0,\n",
       "  'bag': 0,\n",
       "  'balanced': 1,\n",
       "  'ban': 0,\n",
       "  'banknotes': 1,\n",
       "  'banned': 1,\n",
       "  'barrier': 1,\n",
       "  'bart': 0,\n",
       "  'bartholomew': 1,\n",
       "  'bartâ€™s': 0,\n",
       "  'based': 1,\n",
       "  'basketball': 0,\n",
       "  'bay': 2,\n",
       "  'bayside': 0,\n",
       "  'bc': 1,\n",
       "  'bcc': 1,\n",
       "  'be': 19,\n",
       "  'beach': 1,\n",
       "  'beaches': 0,\n",
       "  'beats': 0,\n",
       "  'beattie': 1,\n",
       "  'beautiful': 0,\n",
       "  'beauty': 0,\n",
       "  'became': 1,\n",
       "  'because': 5,\n",
       "  'beckmanns': 1,\n",
       "  'become': 0,\n",
       "  'been': 13,\n",
       "  'beenleigh': 1,\n",
       "  'beenleighredland': 1,\n",
       "  'before': 8,\n",
       "  'began': 0,\n",
       "  'begun': 1,\n",
       "  'behaviour': 1,\n",
       "  'behind': 1,\n",
       "  'beholden': 2,\n",
       "  'being': 4,\n",
       "  'believe': 3,\n",
       "  'believes': 0,\n",
       "  'bellzwebster': 0,\n",
       "  'below': 2,\n",
       "  'benefit': 0,\n",
       "  'benefits': 0,\n",
       "  'benjaminwt': 0,\n",
       "  'berkman': 1,\n",
       "  'berths': 0,\n",
       "  'best': 3,\n",
       "  'better': 2,\n",
       "  'between': 3,\n",
       "  'bicyclemackay': 3,\n",
       "  'bicycleqld': 3,\n",
       "  'big': 7,\n",
       "  'biggest': 0,\n",
       "  'bikeway': 1,\n",
       "  'bikeways': 4,\n",
       "  'bill': 2,\n",
       "  'billion': 14,\n",
       "  'billionaire': 1,\n",
       "  'billions': 3,\n",
       "  'bills': 1,\n",
       "  'billypinker': 0,\n",
       "  'bindiirwin': 0,\n",
       "  'binned': 1,\n",
       "  'bioenergy': 0,\n",
       "  'biofutures': 0,\n",
       "  'biologist': 0,\n",
       "  'birds': 0,\n",
       "  'bitch': 1,\n",
       "  'bitumen': 1,\n",
       "  'black': 1,\n",
       "  'blame': 1,\n",
       "  'blitzing': 0,\n",
       "  'blue': 1,\n",
       "  'bluey': 0,\n",
       "  'blunder': 1,\n",
       "  'bob': 0,\n",
       "  'boilermaker': 0,\n",
       "  'bold': 1,\n",
       "  'bonney': 2,\n",
       "  'booking': 0,\n",
       "  'boom': 1,\n",
       "  'boost': 3,\n",
       "  'booth': 0,\n",
       "  'booths': 0,\n",
       "  'borbidge': 2,\n",
       "  'border': 4,\n",
       "  'borders': 7,\n",
       "  'borrow': 3,\n",
       "  'borrowings': 0,\n",
       "  'boss': 1,\n",
       "  'both': 1,\n",
       "  'bounce': 0,\n",
       "  'bowen': 0,\n",
       "  'bowens': 0,\n",
       "  'bowling': 0,\n",
       "  'boys': 1,\n",
       "  'bradfield': 2,\n",
       "  'bragging': 1,\n",
       "  'branch': 2,\n",
       "  'brazen00': 2,\n",
       "  'breaches': 2,\n",
       "  'breaking': 6,\n",
       "  'breenie9': 1,\n",
       "  'breeze': 2,\n",
       "  'brew': 0,\n",
       "  'bribie': 0,\n",
       "  'brichardson1964': 0,\n",
       "  'bridge': 2,\n",
       "  'bridgettejorja': 0,\n",
       "  'brilliant': 2,\n",
       "  'bring': 0,\n",
       "  'bris': 2,\n",
       "  'brisbane': 6,\n",
       "  'brisbanebroncos': 0,\n",
       "  'brisbanelions': 0,\n",
       "  'brisbanewestbug': 3,\n",
       "  'brisbaneâ€™s': 0,\n",
       "  'broadbeach': 1,\n",
       "  'broadwater': 1,\n",
       "  'brothers': 0,\n",
       "  'brought': 1,\n",
       "  'bruce': 38,\n",
       "  'brutal': 1,\n",
       "  'bryan': 1,\n",
       "  'bub': 0,\n",
       "  'buckle': 0,\n",
       "  'buckled': 1,\n",
       "  'budget': 7,\n",
       "  'budgets': 1,\n",
       "  'bugnorth': 3,\n",
       "  'build': 11,\n",
       "  'building': 4,\n",
       "  'builds': 1,\n",
       "  'built': 3,\n",
       "  'bull': 2,\n",
       "  'bundaberg': 0,\n",
       "  'bundabergdrinks': 0,\n",
       "  'bundamba': 0,\n",
       "  'bundlesnl': 1,\n",
       "  'burleigh': 5,\n",
       "  'bus': 2,\n",
       "  'buses': 1,\n",
       "  'bushwalk': 0,\n",
       "  'busiest': 1,\n",
       "  'business': 3,\n",
       "  'businesses': 4,\n",
       "  'bust': 1,\n",
       "  'busted': 1,\n",
       "  'busway': 1,\n",
       "  'busâ€™': 1,\n",
       "  'but': 21,\n",
       "  'buy': 0,\n",
       "  'by': 32,\n",
       "  'cableway': 0,\n",
       "  'caesar': 0,\n",
       "  'cafe': 0,\n",
       "  'cafÃ©': 0,\n",
       "  'cairns': 3,\n",
       "  'cajeffrey14': 1,\n",
       "  'calamvale': 0,\n",
       "  'call': 0,\n",
       "  'called': 2,\n",
       "  'calls': 1,\n",
       "  'caloundra': 2,\n",
       "  'came': 0,\n",
       "  'cameras': 1,\n",
       "  'camerondickqld': 10,\n",
       "  'campaign': 9,\n",
       "  'campaigners': 2,\n",
       "  'campaigning': 4,\n",
       "  'campaignlife': 0,\n",
       "  'campaignâ€': 1,\n",
       "  'campaignğŸï¸ğŸ”¥': 0,\n",
       "  'campbell': 2,\n",
       "  'campbellnewman': 14,\n",
       "  'campbellnewmanâ€™s': 3,\n",
       "  'campradt': 1,\n",
       "  'camsmith9': 0,\n",
       "  'can': 16,\n",
       "  'canada': 3,\n",
       "  'canberra': 3,\n",
       "  'candidate': 2,\n",
       "  'candidates': 3,\n",
       "  'cannot': 0,\n",
       "  'cant': 1,\n",
       "  'canâ€™t': 5,\n",
       "  'cap': 1,\n",
       "  'capacity': 1,\n",
       "  'car': 2,\n",
       "  'card': 2,\n",
       "  'cards': 2,\n",
       "  'care': 2,\n",
       "  'careers': 0,\n",
       "  'cares': 0,\n",
       "  'caretaker': 0,\n",
       "  'carl': 0,\n",
       "  'carole': 1,\n",
       "  'carrie': 0,\n",
       "  'case': 0,\n",
       "  'cases': 24,\n",
       "  'cassowary': 0,\n",
       "  'cast': 1,\n",
       "  'casualty': 0,\n",
       "  'catch': 1,\n",
       "  'catching': 0,\n",
       "  'catedempsey': 0,\n",
       "  'caught': 1,\n",
       "  'cbdbug': 4,\n",
       "  'cdstrunk': 0,\n",
       "  'cent': 1,\n",
       "  'centre': 0,\n",
       "  'centrepiece': 1,\n",
       "  'centres': 0,\n",
       "  'centuryyuasa': 0,\n",
       "  'chains': 1,\n",
       "  'challenges': 0,\n",
       "  'chance': 0,\n",
       "  'change': 5,\n",
       "  'changed': 1,\n",
       "  'changer': 1,\n",
       "  'changerooms': 0,\n",
       "  'channel': 0,\n",
       "  'chaos': 1,\n",
       "  'chaosqldvotes': 0,\n",
       "  'chaotic': 2,\n",
       "  'chapter': 1,\n",
       "  'charge': 1,\n",
       "  'charged': 2,\n",
       "  'charismullenmp': 0,\n",
       "  'charters': 4,\n",
       "  'chats': 1,\n",
       "  'chatted': 1,\n",
       "  'chatting': 0,\n",
       "  'check': 1,\n",
       "  'checking': 0,\n",
       "  'chief': 0,\n",
       "  'child': 0,\n",
       "  'children': 0,\n",
       "  'chill': 0,\n",
       "  'choice': 7,\n",
       "  'choose': 1,\n",
       "  'chris4coomera': 1,\n",
       "  'christian': 1,\n",
       "  'church': 0,\n",
       "  'ciaraejones': 2,\n",
       "  'city': 0,\n",
       "  'civil': 1,\n",
       "  'ck': 1,\n",
       "  'claims': 1,\n",
       "  'clarebarnes10': 1,\n",
       "  'clean': 3,\n",
       "  'cleannrgcouncil': 1,\n",
       "  'clear': 3,\n",
       "  'clearing': 3,\n",
       "  'clearly': 1,\n",
       "  'clevelandredland': 1,\n",
       "  'climate': 2,\n",
       "  'clive': 14,\n",
       "  'clivefpalmer': 2,\n",
       "  'cliveâ€™s': 1,\n",
       "  'close': 2,\n",
       "  'closed': 1,\n",
       "  'closer': 0,\n",
       "  'cloth': 1,\n",
       "  'club': 1,\n",
       "  'clubhouse': 1,\n",
       "  'clubs': 0,\n",
       "  'coal': 1,\n",
       "  'coalition': 3,\n",
       "  'coast': 12,\n",
       "  'coastâ€™s': 0,\n",
       "  'coffee': 0,\n",
       "  'cold': 2,\n",
       "  'colincaudell': 4,\n",
       "  'collapsed': 1,\n",
       "  'collar': 1,\n",
       "  'colleague': 4,\n",
       "  'college': 0,\n",
       "  'colourful': 0,\n",
       "  'come': 3,\n",
       "  'comes': 0,\n",
       "  'coming': 1,\n",
       "  'comments': 1,\n",
       "  'commissioned': 0,\n",
       "  'commitment': 5,\n",
       "  'commitments': 5,\n",
       "  'committed': 2,\n",
       "  'committing': 0,\n",
       "  'commonheather': 0,\n",
       "  'communities': 0,\n",
       "  'community': 2,\n",
       "  'company': 3,\n",
       "  'complete': 0,\n",
       "  'completed': 1,\n",
       "  'compliant': 2,\n",
       "  'comprehensive': 0,\n",
       "  'compulsory': 1,\n",
       "  'concerns': 0,\n",
       "  'concessional': 0,\n",
       "  'conducted': 5,\n",
       "  'conference': 1,\n",
       "  'conferences': 0,\n",
       "  'confessed': 1,\n",
       "  'confirmed': 12,\n",
       "  'confirms': 1,\n",
       "  'congestion': 1,\n",
       "  'congrats': 1,\n",
       "  'congratulations': 1,\n",
       "  'connector': 0,\n",
       "  'conscious': 1,\n",
       "  'conservation': 0,\n",
       "  'considering': 0,\n",
       "  'constituents': 1,\n",
       "  'construction': 2,\n",
       "  'consultations': 0,\n",
       "  'contact': 1,\n",
       "  'contain': 1,\n",
       "  'container': 0,\n",
       "  'containers': 0,\n",
       "  'content': 1,\n",
       "  'contested': 1,\n",
       "  'continue': 1,\n",
       "  'continues': 0,\n",
       "  'contracting': 1,\n",
       "  'contractors': 0,\n",
       "  'contradicted': 1,\n",
       "  'contrast': 0,\n",
       "  'conventional': 1,\n",
       "  'conway': 0,\n",
       "  'cookdâ€™aguilar': 1,\n",
       "  'coolangatta': 0,\n",
       "  'coomera': 0,\n",
       "  'corinda': 0,\n",
       "  'corner': 0,\n",
       "  'coronavirus': 6,\n",
       "  'cost': 0,\n",
       "  'costings': 10,\n",
       "  'costs': 0,\n",
       "  'costumes': 0,\n",
       "  'could': 3,\n",
       "  'couldnâ€™t': 1,\n",
       "  'councillors': 0,\n",
       "  'councils': 0,\n",
       "  'count': 3,\n",
       "  'counting': 1,\n",
       "  'countries': 0,\n",
       "  'country': 0,\n",
       "  'couple': 0,\n",
       "  'courage': 0,\n",
       "  'courageous': 1,\n",
       "  'couriermail': 2,\n",
       "  'course': 1,\n",
       "  'courses': 0,\n",
       "  'courtesy': 0,\n",
       "  'cover': 0,\n",
       "  'covid': 4,\n",
       "  'covid19': 18,\n",
       "  'covid19au': 2,\n",
       "  'cowboys': 0,\n",
       "  'cowboysleagues': 0,\n",
       "  'coxyjindas': 2,\n",
       "  'cq': 1,\n",
       "  'craigcrawfordmp': 2,\n",
       "  'crashing': 0,\n",
       "  'create': 8,\n",
       "  'created': 1,\n",
       "  'creating': 2,\n",
       "  'creation': 0,\n",
       "  'credit': 1,\n",
       "  'credits': 0,\n",
       "  'crew': 0,\n",
       "  'crisis': 0,\n",
       "  'critical': 1,\n",
       "  'critically': 0,\n",
       "  'criticising': 1,\n",
       "  'critters': 0,\n",
       "  'crleswalker': 0,\n",
       "  'crops': 0,\n",
       "  'cross': 1,\n",
       "  'crownofthorns': 0,\n",
       "  'cruise': 3,\n",
       "  'cruises': 1,\n",
       "  'crumbed': 1,\n",
       "  'crunching': 1,\n",
       "  'cuddling': 0,\n",
       "  'cultural': 0,\n",
       "  'culture': 1,\n",
       "  'cup': 0,\n",
       "  'current': 1,\n",
       "  'currumbin': 3,\n",
       "  'curtispittmp': 1,\n",
       "  'cut': 21,\n",
       "  'cuts': 27,\n",
       "  'cutting': 4,\n",
       "  'cwhfau': 0,\n",
       "  'cwslive': 0,\n",
       "  'cycleway': 1,\n",
       "  'cycleways': 2,\n",
       "  'daily': 1,\n",
       "  'daisy': 1,\n",
       "  'daleypearson': 0,\n",
       "  'dam': 0,\n",
       "  'damage': 1,\n",
       "  'dams': 1,\n",
       "  'dancing': 1,\n",
       "  'data': 1,\n",
       "  'date': 1,\n",
       "  'davethebass': 1,\n",
       "  'david': 2,\n",
       "  'davidcrisafulli': 3,\n",
       "  'dawe': 1,\n",
       "  'day': 7,\n",
       "  'days': 8,\n",
       "  'de': 0,\n",
       "  'deal': 1,\n",
       "  'death': 1,\n",
       "  'deaths': 1,\n",
       "  'deb': 18,\n",
       "  'debate': 3,\n",
       "  'debfrecklington': 76,\n",
       "  'debfrecklingtons': 1,\n",
       "  'debfrecklingtonâ€™s': 2,\n",
       "  'debfrecklington\\u2069': 1,\n",
       "  'debilitating': 0,\n",
       "  'debritz': 1,\n",
       "  'debs': 2,\n",
       "  'debâ€™s': 1,\n",
       "  'deceitful': 1,\n",
       "  'december': 0,\n",
       "  'deciders': 0,\n",
       "  'decimated': 1,\n",
       "  'decision': 1,\n",
       "  'decisions': 1,\n",
       "  'decisive': 0,\n",
       "  'decriminalisation': 1,\n",
       "  'dee': 1,\n",
       "  'deeem777': 0,\n",
       "  'deemadigan': 2,\n",
       "  'deeper': 1,\n",
       "  'deeply': 1,\n",
       "  'defeat': 1,\n",
       "  'delicious': 0,\n",
       "  'delish': 0,\n",
       "  'deliver': 5,\n",
       "  'delivered': 2,\n",
       "  'delivering': 0,\n",
       "  'delivers': 1,\n",
       "  'delivery': 0,\n",
       "  'delma': 0,\n",
       "  'deluge': 1,\n",
       "  'democracy': 0,\n",
       "  'democracysausage': 0,\n",
       "  'denied': 1,\n",
       "  'denouncing': 1,\n",
       "  'department': 1,\n",
       "  'deputy': 3,\n",
       "  'deserve': 1,\n",
       "  'deserves': 2,\n",
       "  'designing': 1,\n",
       "  'despite': 1,\n",
       "  'destination': 0,\n",
       "  'destinations': 0,\n",
       "  'destined': 0,\n",
       "  'destroying': 0,\n",
       "  'detail': 0,\n",
       "  'details': 0,\n",
       "  'detected': 0,\n",
       "  'detection': 1,\n",
       "  'detour': 0,\n",
       "  'devastated': 1,\n",
       "  'develop': 1,\n",
       "  'developed': 0,\n",
       "  'developers': 1,\n",
       "  'developing': 0,\n",
       "  'development': 1,\n",
       "  'devils': 0,\n",
       "  'diabolical': 1,\n",
       "  'did': 13,\n",
       "  'didnâ€™t': 3,\n",
       "  'died': 5,\n",
       "  'dipping': 0,\n",
       "  'direct': 2,\n",
       "  'direction': 0,\n",
       "  'directly': 2,\n",
       "  'director': 2,\n",
       "  'disability': 2,\n",
       "  'discount': 1,\n",
       "  'discovered': 0,\n",
       "  'disgruntled': 1,\n",
       "  'dishonesty': 1,\n",
       "  'disperse': 1,\n",
       "  'distance': 1,\n",
       "  'distancing': 1,\n",
       "  'dna': 6,\n",
       "  'do': 11,\n",
       "  'dockside': 1,\n",
       "  'doctor': 1,\n",
       "  'doctors': 0,\n",
       "  'document': 0,\n",
       "  'dodginess': 1,\n",
       "  'dodging': 1,\n",
       "  'does': 2,\n",
       "  'doesnt': 2,\n",
       "  'doesnâ€™t': 5,\n",
       "  'doing': 4,\n",
       "  'dollar': 1,\n",
       "  'don': 1,\n",
       "  'donations': 1,\n",
       "  'donbrownmp': 3,\n",
       "  'done': 10,\n",
       "  'dont': 2,\n",
       "  'dontriskdeb': 1,\n",
       "  'donâ€™t': 8,\n",
       "  'doors': 0,\n",
       "  'doorstep': 0,\n",
       "  'double': 1,\n",
       "  'down': 13,\n",
       "  'downgraded': 1,\n",
       "  'downgrades': 1,\n",
       "  'downsdawson': 1,\n",
       "  'dozens': 0,\n",
       "  'dr': 1,\n",
       "  'dranthonylynham': 1,\n",
       "  'draw': 1,\n",
       "  'drawings': 0,\n",
       "  'draws': 1,\n",
       "  'drcrowanmp': 1,\n",
       "  'dress': 0,\n",
       "  'drinking': 0,\n",
       "  'drive': 1,\n",
       "  'driven': 1,\n",
       "  'drivers': 2,\n",
       "  'driving': 1,\n",
       "  'dropped': 0,\n",
       "  'drops': 1,\n",
       "  'due': 0,\n",
       "  'dump': 1,\n",
       "  'dumped': 3,\n",
       "  'duncan': 1,\n",
       "  'duncanpeggmp': 2,\n",
       "  'duplic': 1,\n",
       "  'duplicate': 1,\n",
       "  'duplicating': 1,\n",
       "  'duplication': 3,\n",
       "  'durack': 0,\n",
       "  'during': 2,\n",
       "  'dusted': 0,\n",
       "  'duties': 1,\n",
       "  'dying': 5,\n",
       "  'dâ€™aguilar': 1,\n",
       "  'each': 0,\n",
       "  'ear': 0,\n",
       "  'earlier': 1,\n",
       "  'early': 1,\n",
       "  'earthmoving': 1,\n",
       "  'ease': 1,\n",
       "  'easily': 2,\n",
       "  'easing': 0,\n",
       "  'east': 1,\n",
       "  'eastbug': 3,\n",
       "  'easy': 0,\n",
       "  'economic': 18,\n",
       "  'economies': 0,\n",
       "  'economy': 3,\n",
       "  'edmonton': 2,\n",
       "  'education': 2,\n",
       "  'effective': 3,\n",
       "  'effects': 0,\n",
       "  'effort': 0,\n",
       "  'eight': 0,\n",
       "  'elected': 7,\n",
       "  'election': 22,\n",
       "  'electionâ€™s': 1,\n",
       "  'electorate': 1,\n",
       "  'electric': 1,\n",
       "  'electrical': 0,\n",
       "  'electricity': 0,\n",
       "  'electrivenet': 1,\n",
       "  'elliotjstein': 1,\n",
       "  'embarrassing': 1,\n",
       "  'embarrassingly': 1,\n",
       "  'embarrassment': 1,\n",
       "  'emeleus': 1,\n",
       "  'emerges': 1,\n",
       "  'emissions': 2,\n",
       "  'employ': 1,\n",
       "  'employed': 0,\n",
       "  'employing': 0,\n",
       "  'employment': 0,\n",
       "  'empowering': 0,\n",
       "  'empty': 1,\n",
       "  'encouraging': 0,\n",
       "  'end': 2,\n",
       "  'endangered': 1,\n",
       "  'endorsed': 1,\n",
       "  'energy': 5,\n",
       "  'engine': 0,\n",
       "  'englandâ€™s': 1,\n",
       "  'enough': 0,\n",
       "  'ensure': 0,\n",
       "  'enter': 0,\n",
       "  'entire': 0,\n",
       "  'entirely': 1,\n",
       "  'environment': 1,\n",
       "  'environmental': 0,\n",
       "  'environmentally': 1,\n",
       "  'episode': 0,\n",
       "  'equating': 1,\n",
       "  'equipment': 0,\n",
       "  'equivalent': 0,\n",
       "  'ergonenergy': 1,\n",
       "  'ersqueensland': 2,\n",
       "  'especially': 2,\n",
       "  ...}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df_AnnastaciaMP = tweets_df.filter(pl.col(\"username\") == \"AnnastaciaMP\")\n",
    "tweets_df_MarkBaileyMP = tweets_df.filter(pl.col(\"username\") == \"MarkBaileyMP\")\n",
    "docframe.compute_token_frequencies({'tweets_df_AnnastaciaMP': tweets_df_AnnastaciaMP, 'tweets_df_MarkBaileyMP': tweets_df_MarkBaileyMP})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d35dd555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DocFrame is ready! ğŸš€\n",
      "Polars version: 1.31.0\n",
      "Text namespace registered for enhanced text processing\n",
      "Text namespace available: True\n"
     ]
    }
   ],
   "source": [
    "# Essential imports\n",
    "import polars as pl\n",
    "import docframe  # This registers the text namespace\n",
    "from docframe import DocDataFrame, DocLazyFrame\n",
    "\n",
    "# Optional: for better display formatting\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"DocFrame is ready! ğŸš€\")\n",
    "print(f\"Polars version: {pl.__version__}\")\n",
    "print(\"Text namespace registered for enhanced text processing\")\n",
    "\n",
    "# Quick validation that text namespace is available\n",
    "sample_df = pl.DataFrame({\"text\": [\"hello world\", \"test text\"]})\n",
    "print(f\"Text namespace available: {hasattr(sample_df.get_column('text'), 'text')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bf0cd4",
   "metadata": {},
   "source": [
    "## 2. Loading Real-World Data\n",
    "\n",
    "We'll use two datasets from the 2020 Queensland election:\n",
    "- **Candidate Information**: Details about political candidates including gender\n",
    "- **Tweet Data**: Over 6,000 tweets from candidates during the campaign\n",
    "\n",
    "Let's see how DocFrame automatically detects the main text columns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c84ca86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—³ï¸ Candidate Information Dataset\n",
      "Shape: (133, 8)\n",
      "Automatically detected document column: 'evidence'\n",
      "Available columns: ['party', 'electorate', 'first_name', 'last_name', 'twitter_url', 'username', 'gender', 'evidence']\n",
      "\n",
      "Sample data:\n",
      "Document column: 'evidence'\n",
      "shape: (3, 5)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ party                    â”† first_name â”† last_name â”† gender â”† evidence                        â”‚\n",
      "â”‚ ---                      â”† ---        â”† ---       â”† ---    â”† ---                             â”‚\n",
      "â”‚ str                      â”† str        â”† str       â”† str    â”† str                             â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ Informed Medical Options â”† Marilyn    â”† Winters   â”† F      â”† \"THREE Redlands women from theâ€¦ â”‚\n",
      "â”‚ (IMOPâ€¦                   â”†            â”†           â”†        â”†                                 â”‚\n",
      "â”‚ Informed Medical Options â”† Tara       â”† Garozzo   â”† F      â”† \"Ms GarozzoÂ…\" (https://www.daiâ€¦ â”‚\n",
      "â”‚ (IMOPâ€¦                   â”†            â”†           â”†        â”†                                 â”‚\n",
      "â”‚ Informed Medical Options â”† Allona     â”† Lahn      â”† F      â”† \"A country girl at heartÂ…\" (htâ€¦ â”‚\n",
      "â”‚ (IMOPâ€¦                   â”†            â”†           â”†        â”†                                 â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "Evidence column samples (auto-detected document column):\n",
      "1. \"THREE Redlands women from the Informed Medical Options PartyÂ…\" (https://www.redlandcitybulletin.com...\n",
      "2. \"Ms GarozzoÂ…\" (https://www.dailymail.co.uk/news/article-12240405/Innisfail-Cardwell-Tully-lresidents...\n",
      "3. \"A country girl at heartÂ…\" (https://allona.com.au/)...\n"
     ]
    }
   ],
   "source": [
    "# Load candidate information data\n",
    "candidates_df = docframe.read_csv(\"data/ADO/candidate_info_gender.csv\")\n",
    "\n",
    "print(\"ğŸ—³ï¸ Candidate Information Dataset\")\n",
    "print(f\"Shape: {candidates_df.shape}\")\n",
    "print(f\"Automatically detected document column: '{candidates_df.active_document_name}'\")\n",
    "print(f\"Available columns: {candidates_df.columns}\")\n",
    "print()\n",
    "\n",
    "# Show first few rows\n",
    "print(\"Sample data:\")\n",
    "print(\n",
    "    candidates_df.head(3).select(\n",
    "        [\"party\", \"first_name\", \"last_name\", \"gender\", \"evidence\"]\n",
    "    )\n",
    ")\n",
    "print()\n",
    "\n",
    "# Let's see what the evidence column (auto-detected as document) contains\n",
    "print(\"Evidence column samples (auto-detected document column):\")\n",
    "for i, text in enumerate(candidates_df.document.head(3).to_list()):\n",
    "    print(f\"{i + 1}. {text[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33195de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦ Tweet Dataset\n",
      "Shape: (2380, 15)\n",
      "Automatically detected document column: 'text'\n",
      "Available columns: ['tweet_id', 'created_at', 'retrieved_at', 'user_id', 'username', 'text', 'retweeted_tweet_id', 'retweeted_user_id', 'retweeted_user_name', 'in_reply_to_tweet_id', 'in_reply_to_user_id', 'in_reply_to_user_name', 'quoted_tweet_id', 'quoted_user_id', 'quoted_user_name']\n",
      "\n",
      "Sample tweets:\n",
      "1. RT @GhostWhoVotes: #Newspoll QLD State 2 Party Preferred: ALP 52 (+3 since July) LNP 48 (-3) #qldvotes #auspol\n",
      "   --------------------------------------------------------------------------------\n",
      "2. RT @camerondickqld: And @DebFrecklington &amp; @TimManderMP  your ideologically-driven obsession with a surplus will lead to significant economic problems - 30,000 Queenslanders sacked, sky-high unemployment, health services cut, regional communities punished &amp; a longer and deeper recession.\n",
      "   --------------------------------------------------------------------------------\n",
      "3. RT @QLDLabor: Different election, same rubbish.\n",
      "\n",
      "There is no death tax.\n",
      "\n",
      "Deb Frecklington needs to show some leadership and call this fake news out for what it is - a lie.\n",
      "\n",
      "#qldvotes #qldpol\n",
      "\n",
      "https://t.co/DjrkugdTt6\n",
      "   --------------------------------------------------------------------------------\n",
      "\n",
      "Dataset Overview:\n",
      "Total tweets: 2,380\n",
      "Unique users: 79\n"
     ]
    },
    {
     "ename": "DuplicateError",
     "evalue": "the name 'created_at' is duplicate\n\nIt's possible that multiple expressions are returning the same default column name. If this is the case, try renaming the columns with `.alias(\"new_name\")` to avoid duplicate column names.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDuplicateError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal tweets: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(tweets_df)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnique users: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtweets_df.select(pl.col(\u001b[33m'\u001b[39m\u001b[33musername\u001b[39m\u001b[33m'\u001b[39m)).n_unique()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDate range: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mtweets_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcreated_at\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcreated_at\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     23\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Sources/docframe/docframe/core/docframe.py:575\u001b[39m, in \u001b[36mDocDataFrame.__getattr__.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m575\u001b[39m     result = \u001b[43mattr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    576\u001b[39m     \u001b[38;5;66;03m# If result is a DataFrame and contains our document column, wrap it\u001b[39;00m\n\u001b[32m    577\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    578\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(result, pl.DataFrame)\n\u001b[32m    579\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._document_column_name \u001b[38;5;129;01min\u001b[39;00m result.columns\n\u001b[32m    580\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/general/lib/python3.13/site-packages/polars/dataframe/frame.py:9833\u001b[39m, in \u001b[36mDataFrame.select\u001b[39m\u001b[34m(self, *exprs, **named_exprs)\u001b[39m\n\u001b[32m   9731\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   9732\u001b[39m \u001b[33;03mSelect columns from this DataFrame.\u001b[39;00m\n\u001b[32m   9733\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   9826\u001b[39m \u001b[33;03mâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\u001b[39;00m\n\u001b[32m   9827\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   9828\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpolars\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlazyframe\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mopt_flags\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m QueryOptFlags\n\u001b[32m   9830\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m   9831\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   9832\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mnamed_exprs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m9833\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mQueryOptFlags\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_eager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   9834\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/general/lib/python3.13/site-packages/polars/_utils/deprecation.py:97\u001b[39m, in \u001b[36mdeprecate_streaming_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     93\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mengine\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33min-memory\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33mstreaming\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/general/lib/python3.13/site-packages/polars/lazyframe/opt_flags.py:330\u001b[39m, in \u001b[36mforward_old_opt_flags.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m         optflags = cb(optflags, kwargs.pop(key))  \u001b[38;5;66;03m# type: ignore[no-untyped-call,unused-ignore]\u001b[39;00m\n\u001b[32m    329\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33moptimizations\u001b[39m\u001b[33m\"\u001b[39m] = optflags\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/general/lib/python3.13/site-packages/polars/lazyframe/frame.py:2332\u001b[39m, in \u001b[36mLazyFrame.collect\u001b[39m\u001b[34m(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, collapse_joins, no_optimization, engine, background, optimizations, **_kwargs)\u001b[39m\n\u001b[32m   2330\u001b[39m \u001b[38;5;66;03m# Only for testing purposes\u001b[39;00m\n\u001b[32m   2331\u001b[39m callback = _kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mpost_opt_callback\u001b[39m\u001b[33m\"\u001b[39m, callback)\n\u001b[32m-> \u001b[39m\u001b[32m2332\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(\u001b[43mldf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mDuplicateError\u001b[39m: the name 'created_at' is duplicate\n\nIt's possible that multiple expressions are returning the same default column name. If this is the case, try renaming the columns with `.alias(\"new_name\")` to avoid duplicate column names."
     ]
    }
   ],
   "source": [
    "# Load tweet data - this is the main dataset for text analysis\n",
    "tweets_df = docframe.read_csv(\"data/ADO/qldelection2020_candidate_tweets.csv\")\n",
    "\n",
    "print(\"ğŸ¦ Tweet Dataset\")\n",
    "print(f\"Shape: {tweets_df.shape}\")\n",
    "print(f\"Automatically detected document column: '{tweets_df.active_document_name}'\")\n",
    "print(f\"Available columns: {tweets_df.columns}\")\n",
    "print()\n",
    "\n",
    "# Show sample tweets\n",
    "print(\"Sample tweets:\")\n",
    "for i, tweet in enumerate(tweets_df.document.head(3).to_list()):\n",
    "    print(f\"{i + 1}. {tweet}\")\n",
    "    print(\"   \" + \"-\" * 80)\n",
    "print()\n",
    "\n",
    "# Show dataset info using DocFrame built-in method\n",
    "print(\"Dataset Overview:\")\n",
    "print(f\"Total tweets: {len(tweets_df):,}\")\n",
    "print(f\"Unique users: {tweets_df.select(pl.col('username')).n_unique()}\")\n",
    "print(\n",
    "    f\"Date range: {tweets_df.select(pl.col('created_at').min(), pl.col('created_at').max())}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2171e23",
   "metadata": {},
   "source": [
    "## 3. DocFrame Text Namespace: Unified Text Operations\n",
    "\n",
    "One of DocFrame' key innovations is the **text namespace** - a unified API that works across all Polars data types (Series, DataFrames, Expressions). The same `.text` methods work everywhere!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb604ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text namespace works on Series, DataFrames, and Expressions uniformly!\n",
    "\n",
    "print(\"ğŸ”¤ Text Namespace Demonstration\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. On individual Series (traditional approach)\n",
    "sample_text = tweets_df.document.head(1)\n",
    "print(\"1. Text operations on Series:\")\n",
    "print(f\"   Text: {sample_text.to_list()[0][:60]}...\")\n",
    "print(f\"   Word count: {sample_text.text.word_count().to_list()[0]}\")\n",
    "print(f\"   Character count: {sample_text.text.char_count().to_list()[0]}\")\n",
    "print()\n",
    "\n",
    "# 2. On DataFrame expressions (DocFrame magic!)\n",
    "print(\"2. Text operations on DataFrame (vectorized):\")\n",
    "text_stats = tweets_df.head(5).select(\n",
    "    [\n",
    "        pl.col(\"username\"),\n",
    "        pl.col(\"text\"),\n",
    "        pl.col(\"text\").text.word_count().alias(\"words\"),\n",
    "        pl.col(\"text\").text.char_count().alias(\"chars\"),\n",
    "        pl.col(\"text\").text.sentence_count().alias(\"sentences\"),\n",
    "    ]\n",
    ")\n",
    "print(text_stats)\n",
    "print()\n",
    "\n",
    "# 3. Using the document shortcut (DocDataFrame convenience)\n",
    "print(\"3. Using DocDataFrame's document property:\")\n",
    "quick_stats = tweets_df.head(5).select(\n",
    "    [\n",
    "        pl.col(\"username\"),\n",
    "        tweets_df.document.text.word_count().alias(\"words\"),\n",
    "        tweets_df.document.text.char_count().alias(\"chars\"),\n",
    "    ]\n",
    ")\n",
    "print(quick_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcbb946",
   "metadata": {},
   "source": [
    "## 4. Automatic Text Statistics\n",
    "\n",
    "DocFrame provides built-in methods for comprehensive text analysis. Let's analyze our tweet dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3719a3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get comprehensive text statistics using DocFrame' built-in method\n",
    "print(\"ğŸ“Š Comprehensive Text Statistics\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Use DocDataFrame's describe_text method for automatic statistics\n",
    "text_stats = tweets_df.describe_text()\n",
    "print(\"Dataset-wide text statistics:\")\n",
    "print(text_stats)\n",
    "print()\n",
    "\n",
    "# Add text statistics as new columns for further analysis\n",
    "tweets_with_stats = tweets_df.with_columns(\n",
    "    [\n",
    "        pl.col(\"text\").text.word_count().alias(\"word_count\"),\n",
    "        pl.col(\"text\").text.char_count().alias(\"char_count\"),\n",
    "        pl.col(\"text\").text.sentence_count().alias(\"sentence_count\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Sample tweets with computed statistics:\")\n",
    "print(\n",
    "    tweets_with_stats.head(3).select(\n",
    "        [\"username\", \"word_count\", \"char_count\", \"sentence_count\", \"text\"]\n",
    "    )\n",
    ")\n",
    "print()\n",
    "\n",
    "# Analyze by user - who writes the longest tweets?\n",
    "user_stats = (\n",
    "    tweets_with_stats.group_by(\"username\")\n",
    "    .agg(\n",
    "        [\n",
    "            pl.count().alias(\"tweet_count\"),\n",
    "            pl.col(\"word_count\").mean().alias(\"avg_words\"),\n",
    "            pl.col(\"char_count\").mean().alias(\"avg_chars\"),\n",
    "            pl.col(\"word_count\").max().alias(\"max_words\"),\n",
    "        ]\n",
    "    )\n",
    "    .sort(\"avg_words\", descending=True)\n",
    ")\n",
    "\n",
    "print(\"Top 10 users by average words per tweet:\")\n",
    "print(user_stats.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261af8bc",
   "metadata": {},
   "source": [
    "## 5. Text Processing and Cleaning\n",
    "\n",
    "DocFrame provides powerful text cleaning and processing capabilities. Let's clean our tweet data and extract insights!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4ccf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning with DocFrame\n",
    "print(\"ğŸ§¹ Text Cleaning and Processing\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Take a sample tweet and show cleaning steps\n",
    "sample_tweet_df = tweets_df.head(1)\n",
    "original_text = sample_tweet_df.document.to_list()[0]\n",
    "\n",
    "print(f\"Original tweet: {original_text}\")\n",
    "print()\n",
    "\n",
    "# Apply various text cleaning operations using the text namespace\n",
    "cleaned_df = sample_tweet_df.with_columns(\n",
    "    [\n",
    "        # Clean the text: normalize whitespace, remove extra spaces\n",
    "        pl.col(\"text\").text.clean().alias(\"cleaned\"),\n",
    "        # Tokenize into words\n",
    "        pl.col(\"text\").text.tokenize().alias(\"tokens\"),\n",
    "        # Extract word counts after cleaning\n",
    "        pl.col(\"text\").text.clean().text.word_count().alias(\"clean_word_count\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Cleaning results:\")\n",
    "print(f\"Cleaned text: {cleaned_df.get_column('cleaned').to_list()[0]}\")\n",
    "print(\n",
    "    f\"Tokens: {cleaned_df.get_column('tokens').to_list()[0][:10]}...\"\n",
    ")  # First 10 tokens\n",
    "print(\n",
    "    f\"Word count after cleaning: {cleaned_df.get_column('clean_word_count').to_list()[0]}\"\n",
    ")\n",
    "print()\n",
    "\n",
    "# Apply cleaning to larger dataset for analysis\n",
    "print(\"Processing larger dataset...\")\n",
    "cleaned_tweets = tweets_df.head(100).with_columns(\n",
    "    [\n",
    "        pl.col(\"text\").text.clean().alias(\"cleaned_text\"),\n",
    "        pl.col(\"text\").text.word_count().alias(\"original_words\"),\n",
    "        pl.col(\"text\").text.clean().text.word_count().alias(\"cleaned_words\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Cleaning effectiveness:\")\n",
    "cleaning_stats = cleaned_tweets.select(\n",
    "    [\n",
    "        pl.col(\"original_words\").mean().alias(\"avg_original_words\"),\n",
    "        pl.col(\"cleaned_words\").mean().alias(\"avg_cleaned_words\"),\n",
    "        (pl.col(\"original_words\") - pl.col(\"cleaned_words\"))\n",
    "        .mean()\n",
    "        .alias(\"avg_words_removed\"),\n",
    "    ]\n",
    ")\n",
    "print(cleaning_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae16da3",
   "metadata": {},
   "source": [
    "## 6. Document Column Management\n",
    "\n",
    "DocFrame automatically detects your main text column, but you can easily switch between different text columns or manage multiple text fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46f4f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document column management features\n",
    "print(\"ğŸ“ Document Column Management\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Show current document column detection\n",
    "print(f\"Current document column: '{tweets_df.active_document_name}'\")\n",
    "print(f\"Available columns: {tweets_df.columns}\")\n",
    "print()\n",
    "\n",
    "# Let's create a dataset with multiple text columns to demonstrate switching\n",
    "multi_text_df = tweets_df.head(5).with_columns(\n",
    "    [\n",
    "        pl.col(\"text\").alias(\"tweet_content\"),\n",
    "        pl.col(\"username\").alias(\"user_handle\"),\n",
    "        # Create a summary column\n",
    "        (\n",
    "            pl.lit(\"Tweet by @\")\n",
    "            + pl.col(\"username\")\n",
    "            + pl.lit(\": \")\n",
    "            + pl.col(\"text\").str.slice(0, 50)\n",
    "            + pl.lit(\"...\")\n",
    "        ).alias(\"summary\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Created dataset with multiple text columns:\")\n",
    "print(multi_text_df.select([\"user_handle\", \"summary\", \"tweet_content\"]).head(2))\n",
    "print()\n",
    "\n",
    "# Switch document column to different text field\n",
    "doc_with_summary = DocDataFrame(multi_text_df.to_pandas(), document_column=\"summary\")\n",
    "\n",
    "print(f\"Switched document column to: '{doc_with_summary.active_document_name}'\")\n",
    "print(\"Document property now refers to summary:\")\n",
    "for i, doc in enumerate(doc_with_summary.document.head(2).to_list()):\n",
    "    print(f\"{i + 1}. {doc}\")\n",
    "print()\n",
    "\n",
    "# Demonstrate changing document column dynamically\n",
    "doc_with_content = doc_with_summary.set_document(\"tweet_content\")\n",
    "print(f\"Changed document column to: '{doc_with_content.active_document_name}'\")\n",
    "print(\"Now document property refers to tweet content:\")\n",
    "print(f\"Sample: {doc_with_content.document.head(1).to_list()[0][:80]}...\")\n",
    "print()\n",
    "\n",
    "# Show automatic detection for different datasets\n",
    "print(\"Automatic detection on candidate data:\")\n",
    "print(f\"Detected: '{candidates_df.active_document_name}'\")\n",
    "print(\"This chose 'evidence' because it has the longest average text length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92d2747",
   "metadata": {},
   "source": [
    "## 7. Advanced Text Analysis: Document-Term Matrix\n",
    "\n",
    "DocFrame includes built-in support for creating document-term matrices (DTM) for further analysis like topic modeling or text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370a03c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document-Term Matrix creation with DocFrame\n",
    "print(\"ğŸ”¢ Document-Term Matrix Analysis\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "# Take a small sample for DTM demonstration\n",
    "sample_tweets = tweets_df.head(10).select([\"username\", \"text\"])\n",
    "doc_sample = DocDataFrame(sample_tweets.to_pandas())\n",
    "\n",
    "print(f\"Creating DTM from {len(doc_sample)} tweets...\")\n",
    "print()\n",
    "\n",
    "# Create DTM using DocFrame built-in method\n",
    "dtm = doc_sample.to_dtm()\n",
    "\n",
    "print(f\"DTM shape: {dtm.shape}\")\n",
    "print(f\"Vocabulary size: {len(dtm.columns)} unique terms\")\n",
    "print()\n",
    "\n",
    "print(\"Sample of the DTM (first 5 rows, first 10 terms):\")\n",
    "print(dtm.head(5).select(dtm.columns[:10]))\n",
    "print()\n",
    "\n",
    "# Find most common terms across all documents\n",
    "term_frequencies = dtm.sum().transpose(include_header=True)\n",
    "term_freq_df = pl.DataFrame({\n",
    "    \"term\": dtm.columns,\n",
    "    \"frequency\": [dtm[col].sum() for col in dtm.columns]\n",
    "}).sort(\"frequency\", descending=True)\n",
    "\n",
    "print(\"Top 15 most frequent terms across all tweets:\")\n",
    "print(term_freq_df.head(15))\n",
    "print()\n",
    "\n",
    "# Demonstrate text filtering and analysis workflow\n",
    "print(\"Text filtering example:\")\n",
    "long_tweets = tweets_df.filter(\n",
    "    pl.col(\"text\").text.word_count() > 20\n",
    ").head(5)\n",
    "\n",
    "print(f\"Found {len(long_tweets)} tweets with >20 words:\")\n",
    "for i, (user, text) in enumerate(long_tweets.select([\"username\", \"text\"]).iter_rows()):\n",
    "    word_count = DocDataFrame(pl.DataFrame({\"text\": [text]})).document.text.word_count().to_list()[0]\n",
    "    print(f\"{i+1}. @{user} ({word_count} words): {text[:60]}...\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b5df1e",
   "metadata": {},
   "source": [
    "## 8. Lazy Evaluation and Performance\n",
    "\n",
    "DocFrame supports lazy evaluation through DocLazyFrame, giving you the performance benefits of Polars' lazy evaluation while maintaining text analysis capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a77d798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lazy evaluation with DocLazyFrame\n",
    "print(\"âš¡ Lazy Evaluation Performance\")\n",
    "print(\"=\" * 32)\n",
    "\n",
    "# Convert to lazy frame for efficient processing\n",
    "tweets_lazy = DocLazyFrame(tweets_df.to_lazyframe())\n",
    "\n",
    "print(\n",
    "    f\"Created DocLazyFrame with document column: '{tweets_lazy.active_document_name}'\"\n",
    ")\n",
    "print(\"Lazy operations are built up but not executed until collect()\")\n",
    "print()\n",
    "\n",
    "# Build a complex text analysis pipeline\n",
    "analysis_pipeline = (\n",
    "    tweets_lazy.with_columns(\n",
    "        [\n",
    "            # Add text statistics\n",
    "            pl.col(\"text\").text.word_count().alias(\"word_count\"),\n",
    "            pl.col(\"text\").text.char_count().alias(\"char_count\"),\n",
    "            pl.col(\"text\").text.sentence_count().alias(\"sentence_count\"),\n",
    "            # Clean and process text\n",
    "            pl.col(\"text\").text.clean().alias(\"clean_text\"),\n",
    "        ]\n",
    "    )\n",
    "    .filter(\n",
    "        # Only tweets with substantial content\n",
    "        (pl.col(\"word_count\") >= 5) & (pl.col(\"char_count\") >= 20)\n",
    "    )\n",
    "    .group_by(\"username\")\n",
    "    .agg(\n",
    "        [\n",
    "            pl.count().alias(\"tweet_count\"),\n",
    "            pl.col(\"word_count\").mean().alias(\"avg_words\"),\n",
    "            pl.col(\"char_count\").mean().alias(\"avg_chars\"),\n",
    "            pl.col(\"word_count\").sum().alias(\"total_words\"),\n",
    "        ]\n",
    "    )\n",
    "    .filter(pl.col(\"tweet_count\") >= 3)  # Users with at least 3 tweets\n",
    "    .sort(\"avg_words\", descending=True)\n",
    ")\n",
    "\n",
    "print(\"Built lazy analysis pipeline (not executed yet)\")\n",
    "print(\"Pipeline includes: text stats, cleaning, filtering, grouping, and sorting\")\n",
    "print()\n",
    "\n",
    "# Execute the pipeline\n",
    "print(\"Executing pipeline...\")\n",
    "results = analysis_pipeline.collect()\n",
    "\n",
    "print(f\"Results shape: {results.shape}\")\n",
    "print(\"Top 10 most verbose users (by average words per tweet):\")\n",
    "print(results.head(10))\n",
    "print()\n",
    "\n",
    "# Demonstrate lazy serialization\n",
    "print(\"Lazy serialization:\")\n",
    "lazy_json = tweets_lazy.serialize()\n",
    "print(f\"Serialized lazy frame (first 100 chars): {lazy_json[:100]}...\")\n",
    "\n",
    "# Convert back to eager for final operations\n",
    "final_df = DocDataFrame(results.to_pandas())\n",
    "print(f\"Final DocDataFrame created with {len(final_df)} users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d51a75f",
   "metadata": {},
   "source": [
    "## 9. Data Integration and Serialization\n",
    "\n",
    "DocFrame seamlessly integrates with the broader Python ecosystem and preserves all metadata during serialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cc7188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data integration and serialization\n",
    "print(\"ğŸ”„ Data Integration and Serialization\")\n",
    "print(\"=\" * 37)\n",
    "\n",
    "# 1. Join datasets using document column information\n",
    "print(\"1. Joining candidate and tweet data:\")\n",
    "\n",
    "# Create a small lookup of usernames to candidate info\n",
    "candidate_lookup = candidates_df.select(\n",
    "    [\"username\", \"party\", \"gender\", \"first_name\", \"last_name\"]\n",
    ").filter(pl.col(\"username\").is_not_null())\n",
    "\n",
    "print(f\"Candidate lookup: {len(candidate_lookup)} candidates with Twitter accounts\")\n",
    "print(candidate_lookup.head(3))\n",
    "print()\n",
    "\n",
    "# Join with tweet data to analyze by gender/party\n",
    "enriched_tweets = tweets_df.head(1000).join(\n",
    "    candidate_lookup, on=\"username\", how=\"inner\"\n",
    ")\n",
    "\n",
    "print(f\"Enriched dataset: {len(enriched_tweets)} tweets from candidates\")\n",
    "print()\n",
    "\n",
    "# Analyze tweet patterns by gender\n",
    "gender_analysis = (\n",
    "    enriched_tweets.group_by(\"gender\")\n",
    "    .agg(\n",
    "        [\n",
    "            pl.count().alias(\"tweet_count\"),\n",
    "            pl.col(\"text\").text.word_count().mean().alias(\"avg_words_per_tweet\"),\n",
    "            pl.col(\"text\").text.char_count().mean().alias(\"avg_chars_per_tweet\"),\n",
    "        ]\n",
    "    )\n",
    "    .sort(\"tweet_count\", descending=True)\n",
    ")\n",
    "\n",
    "print(\"Tweet patterns by gender:\")\n",
    "print(gender_analysis)\n",
    "print()\n",
    "\n",
    "# 2. Serialization with metadata preservation\n",
    "print(\"2. Serialization with metadata preservation:\")\n",
    "\n",
    "# Create a rich DocDataFrame with metadata\n",
    "sample_for_serialization = enriched_tweets.head(5)\n",
    "doc_sample = DocDataFrame(sample_for_serialization.to_pandas())\n",
    "\n",
    "print(f\"Original document column: '{doc_sample.active_document_name}'\")\n",
    "\n",
    "# Serialize to JSON (preserves document column info)\n",
    "serialized = doc_sample.serialize(format=\"json\")\n",
    "print(f\"Serialized size: {len(serialized)} characters\")\n",
    "\n",
    "# Deserialize and verify metadata is preserved\n",
    "restored_df = DocDataFrame.deserialize(serialized, format=\"json\")\n",
    "print(f\"Restored document column: '{restored_df.active_document_name}'\")\n",
    "print(f\"Shape preserved: {restored_df.shape == doc_sample.shape}\")\n",
    "print(f\"Columns preserved: {set(restored_df.columns) == set(doc_sample.columns)}\")\n",
    "print()\n",
    "\n",
    "# 3. Export to standard formats while preserving analysis\n",
    "print(\"3. Export capabilities:\")\n",
    "print(\"âœ“ Can export to CSV, Parquet, JSON while preserving computed text statistics\")\n",
    "print(\"âœ“ Document column information can be serialized and restored\")\n",
    "print(\"âœ“ Seamless integration with pandas, scikit-learn, and other tools\")\n",
    "\n",
    "# Example: prepare for machine learning\n",
    "ml_ready = enriched_tweets.select([\"text\", \"gender\", \"party\"]).with_columns(\n",
    "    [\n",
    "        pl.col(\"text\").text.word_count().alias(\"word_count\"),\n",
    "        pl.col(\"text\").text.char_count().alias(\"char_count\"),\n",
    "        pl.col(\"text\").text.sentence_count().alias(\"sentence_count\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"\\\\nML-ready dataset shape: {ml_ready.shape}\")\n",
    "print(\"Ready for classification, clustering, or other ML tasks!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8677af43",
   "metadata": {},
   "source": [
    "## 10. Summary: Why DocFrame?\n",
    "\n",
    "DocFrame transforms text analysis from a complex, multi-step process into an intuitive, unified workflow.\n",
    "\n",
    "### ğŸ¯ Key Benefits Demonstrated:\n",
    "\n",
    "1. **Automatic Intelligence**: No more guessing which column contains your text data\n",
    "2. **Unified API**: Same `.text` methods work everywhere - Series, DataFrames, Expressions\n",
    "3. **Performance**: Built on Polars for speed and memory efficiency\n",
    "4. **Seamless Integration**: Works with your existing data science tools\n",
    "5. **Rich Text Analytics**: Built-in statistics, cleaning, and processing capabilities\n",
    "\n",
    "### ğŸ“Š What We Analyzed:\n",
    "- **6,000+ political tweets** from Queensland election candidates\n",
    "- **133 candidate profiles** with gender and party information\n",
    "- **Text statistics** across different demographics\n",
    "- **Document-term matrices** for advanced analysis\n",
    "- **Lazy evaluation** for performance optimization\n",
    "\n",
    "### ğŸš€ Next Steps:\n",
    "DocFrame is perfect for:\n",
    "- **Social media analysis** (Twitter, Reddit, forums)\n",
    "- **Document processing** (news articles, papers, reports)\n",
    "- **Survey analysis** (open-ended responses, feedback)\n",
    "- **Content analysis** (reviews, comments, descriptions)\n",
    "- **Research workflows** where text is a key data type\n",
    "\n",
    "Try DocFrame in your next text analysis project and experience the difference!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
